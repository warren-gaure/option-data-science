{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> **Livrable n°3 : Captioning d'image** </center>\n",
    "\n",
    "‎ \n",
    "\n",
    "Réalisé par le **groupe n°2** :\n",
    "- BERTHO Lucien\n",
    "- BOSACKI Paul\n",
    "- GAURE Warren\n",
    "- GRENOUILLET Théo\n",
    "- VALLEMONT Hugo\n",
    "\n",
    "\n",
    "‎\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Sommaire**\n",
    "\n",
    "1. [Mise en contexte](#contexte)\n",
    "2. [Objectif du livrable](#objectif)\n",
    "3. [Démarche suivie](#demarche)\n",
    "4. [Importation des bibliothèques](#import)\n",
    "5. [Adaptation pour GPU](#gpu)\n",
    "6. [Chargement et préparation des données](#chargement)\n",
    "7. [Création du modèle de captioning](#model)\n",
    "8. [Entraînement du modèle](#training)\n",
    "9. [Évaluation du modèle](#evaluation)\n",
    "10. [Métriques](#metrics)\n",
    "11. [Conclusion](#conclusion)\n",
    "\n",
    "‎ \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. <a id='contexte'>Mise en contexte</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L’entreprise TouNum est spécialisée dans la numérisation de documents, qu’il s’agisse de textes ou d’images. Ses services sont particulièrement sollicités par des entreprises cherchant à transformer leur base documentaire papier en fichiers numériques exploitables. Aujourd’hui, TouNum souhaite aller plus loin en enrichissant son offre avec des outils basés sur le Machine Learning.\n",
    "\n",
    "En effet, certains clients disposent d’un volume considérable de documents à numériser et expriment un besoin croissant pour des solutions de catégorisation automatique. Une telle innovation leur permettrait d’optimiser le traitement et l’exploitation de leurs données numérisées. Toutefois, TouNum ne dispose pas en interne des compétences nécessaires pour concevoir et mettre en place ces technologies.\n",
    "\n",
    "C’est dans ce cadre que notre équipe de spécialistes en Data Science du CESI est sollicitée. Notre mission consiste à développer une première solution intégrant du captioning automatique : un système capable d’analyser des photographies et de générer une légende descriptive de manière autonome.\n",
    "\n",
    "Heureusement, TouNum possède déjà plusieurs milliers d’images annotées, ce qui constituera une ressource précieuse pour entraîner les modèles de Machine Learning à partir d’un apprentissage supervisé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. <a id='objectif'>Objectif du livrable</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour cette troisième et dernière étape du workflow, il nous est demandé d'ajouter des légendes aux photos qui ont été traitées. Ce livrable propose une méthode de captioning basée sur les réseaux de neurones à convolution (CNN) ainsi que les récurrents (RNN) pour annoter les images fournies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. <a id='demarche'>Démarche suivie</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour ce livrable, nous avons fait le choix de représenter la démarche que nous avons décidé de suivre sous la forme d'un pipeline. Celui-ci représente les diverses grandes étapes que nous avons réalisé pour parvenir au but mentionné dans la partie précédente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"./results/pipeline_l3.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. <a id='import'>Importation des bibliothèques</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import collections\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "#from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. <a id='gpu'>Adaptation pour GPU</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin d'entraîner nos modèles sur le GPU de nos ordinateurs, une configuration est nécéssaire. Celle-ci va optimiser la demande de mémoire pour qu'elle soit allouée de manière croissante. Cela va permettre d'éviter d'allouer directement le maximum dès le début et éviter une surutilisation de celle-ci."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            details = tf.config.experimental.get_device_details(gpu)\n",
    "            print(f\"Nom du GPU détecté : {details.get('device_name', 'Nom inconnu')}\")\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. <a id='chargement'>Chargement et préparation des données</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois les bibliothèques importées et la configuration pour l'utilisation du GPU implémentée, nous pouvons commencer par charger les données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1. <a>Chargement des annotations</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette section, nous chargeons les textes associés avec les images du dataset COCO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ProjectPath = '/tf/'\n",
    "#\n",
    "## Chemin du fichier d'annotations\n",
    "annotation_file = f\"{ProjectPath}dataset_livrable_3/annotations/captions_train2014.json\"\n",
    "#\n",
    "## Chemin du dossier contenant les images à annoter\n",
    "PATH = f\"{ProjectPath}dataset_livrable_3/train2014/train2014/\"\n",
    "\n",
    "# Chemin du fichier d'annotations\n",
    "#annotation_file = \"/tf/dataset_livrable_3/annotations/captions_train2014.json\"\n",
    "\n",
    "# Chemin du dossier contenant les images à annoter\n",
    "#PATH = '/tf/dataset_livrable_3/train2014/train2014/'\n",
    "\n",
    "# Lecture du fichier d'annotation\n",
    "with open(annotation_file, 'r') as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "# Grouper toutes les annotations ayant le meme identifiant.\n",
    "image_path_to_caption = collections.defaultdict(list)\n",
    "for val in annotations['annotations']:\n",
    "    # marquer le debut et la fin de chaq0+ ue annotation\n",
    "    caption = '<start> ' + val['caption'] + ' <end>'\n",
    "    # L'identifiant d'une image fait partie de son chemin d'accès\n",
    "    image_path = PATH + 'COCO_train2014_' + '%012d.jpg' % (val['image_id'])\n",
    "    # Rajout du caption associé à image_path\n",
    "    image_path_to_caption[image_path].append(caption)\n",
    "\n",
    "# Print the first 10 values of image_path_to_caption\n",
    "#for i, (key, value) in enumerate(image_path_to_caption.items()):\n",
    "#    if os.path.exists(key):\n",
    "#        if i == 10:\n",
    "#            break\n",
    "#        print(f\"{key}: {value}\")\n",
    "#        img = Image.open(key)\n",
    "#        plt.imshow(img)\n",
    "#        plt.axis(\"off\")\n",
    "#        plt.show()\n",
    "    \n",
    "# Prendre les premières images seulement\n",
    "image_paths = list(image_path_to_caption.keys())\n",
    "train_image_paths = image_paths[:2000]\n",
    "\n",
    "# Liste de toutes les annotations\n",
    "train_captions = []\n",
    "# Liste de tous les noms de fichiers des images dupliquées (en nombre d'annotations par image)\n",
    "img_name_vector = []\n",
    "\n",
    "for image_path in train_image_paths:\n",
    "    caption_list = image_path_to_caption[image_path]\n",
    "    # Rajout de caption_list dans train_captions\n",
    "    train_captions.extend(caption_list)\n",
    "    # Rajout de image_path dupliquée len(caption_list) fois\n",
    "    img_name_vector.extend([image_path] * len(caption_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "──────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2. <a>Chargement des images</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, nous allons charger et prétraiter les images en créant un fichier contenant les features les plus importantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Telechargement du modèle InceptionV3 pré-entrainé avec la cassification sur ImageNet\n",
    "image_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n",
    "# Creation d'une variable qui sera l'entrée du nouveau modèle de pre-traitement d'images\n",
    "new_input = image_model.input\n",
    "# récupérer la dernière couche caché qui contient l'image en representation compacte\n",
    "hidden_layer = image_model.layers[-1].output\n",
    "\n",
    "# Modèle qui calcule une representation dense des images avec InceptionV3\n",
    "image_features_extract_model = tf.keras.Model(inputs = new_input, outputs = hidden_layer)\n",
    "\n",
    "# Définition de la fonction load_image\n",
    "def load_image(image_path):\n",
    "    \"\"\"\n",
    "    La fonction load_image a pour entrée le chemin d'une image et pour sortie un couple\n",
    "    contenant l'image traitée ainsi que son chemin d'accès.\n",
    "    La fonction load_image effectue les traitement suivant:\n",
    "        1. Chargement du fichier correspondant au chemin d'accès image_path\n",
    "        2. Décodage de l'image en RGB.\n",
    "        3. Redimensionnement de l'image en taille (299, 299).\n",
    "        4. Normalisation des pîxels de l'image entre -1 et 1\n",
    "    \"\"\"\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (299, 299))\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    return img, image_path\n",
    "\n",
    "# Pré-traitement des images\n",
    "# Prendre les noms des images\n",
    "encode_train = sorted(set(img_name_vector))\n",
    "\n",
    "# Creation d'une instance de \"tf.data.Dataset\" partant des noms des images \n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
    "# Division du données en batchs après application du pré-traitement fait par load_image\n",
    "image_dataset = image_dataset.map(\n",
    "  load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(16)\n",
    "\n",
    "# Parcourir le dataset batch par batch pour effectuez le pré-traitement d'InceptionV3\n",
    "for img, path in tqdm(image_dataset):\n",
    "    # Pré-traitement du batch (de taille (16,8,8,2048)) courant par InceptionV3 \n",
    "    batch_features = image_features_extract_model(img)\n",
    "    # Resize du batch de taille (16,8,8,2048) en taille (16,64,2048)\n",
    "    batch_features = tf.reshape(batch_features,\n",
    "                              (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "    # Parcourir le batch courant et stocker le chemin ainsi que le batch avec np.save()\n",
    "    for bf, p in zip(batch_features, path):\n",
    "        path_of_feature = p.numpy().decode(\"utf-8\")\n",
    "        # (chemin de l'image associe a sa nouvelle representation , representation de l'image)\n",
    "        np.save(path_of_feature, bf.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3. <a>Création du tokenizer</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons ici créer un tokenizer. Il s'agit d'un outil qui convertit un texte brut en une séquence de unités de base appelées tokens (mots, sous-mots ou caractères), afin de rendre le texte exploitable par un modèle de machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trouver la taille maximale \n",
    "def calc_max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n",
    "\n",
    "# Chosir les 5000 mots les plus frequents du vocabulaire\n",
    "top_k = 5000\n",
    "#La classe Tokenizer permet de faire du pre-traitement de texte pour reseau de neurones \n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
    "                                                  oov_token=\"<unk>\",\n",
    "                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "# Construit un vocabulaire en se basant sur la liste train_captions\n",
    "tokenizer.fit_on_texts(train_captions)\n",
    "    \n",
    "# Créer le token qui sert à remplir les annotations pour egaliser leurs longueur\n",
    "tokenizer.word_index['<pad>'] = 0\n",
    "tokenizer.index_word[0] = '<pad>'\n",
    "\n",
    "# Creation des vecteurs(liste de token entiers) à partir des annotations (liste de mots)\n",
    "train_seqs = tokenizer.texts_to_sequences(train_captions)\n",
    "\n",
    "# Remplir chaque vecteur à jusqu'à la longueur maximale des annotations\n",
    "cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')\n",
    "\n",
    "# Calcule la longueur maximale qui est utilisée pour stocker les poids d'attention \n",
    "# Elle servira plus tard pour l'affichage lors de l'évaluation\n",
    "max_length = calc_max_length(train_seqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4. <a>Création des jeux d'entraînement</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous crééons ici un dataset d'entraînement par le biais d'un fractionnement 80/20 de notre ensemble de données (80% pour l'entraînement et 20% our la validation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_to_cap_vector = collections.defaultdict(list)\n",
    "# Creation d'un dictionnaire associant les chemins des images avec (fichier .npy) aux annotationss\n",
    "# Les images sont dupliquées car il y a plusieurs annotations par image\n",
    "print(len(img_name_vector), len(cap_vector))\n",
    "for img, cap in zip(img_name_vector, cap_vector):\n",
    "    img_to_cap_vector[img].append(cap)\n",
    "\n",
    "\"\"\"\n",
    "Création des datasets de formation et de validation en utilisant \n",
    "un fractionnement 80-20 de manière aléatoire\n",
    "\"\"\" \n",
    "# Prendre les clés (noms des fichiers d'images traites), *celles-ci ne seront pas dupliquées*\n",
    "img_keys = list(img_to_cap_vector.keys())\n",
    "# Diviser des indices en entrainement et test\n",
    "slice_index = int(len(img_keys) * 0.8)\n",
    "img_name_train_keys = img_keys[:slice_index]\n",
    "img_name_val_keys = img_keys[slice_index:]\n",
    "\n",
    "\"\"\"\n",
    "Les jeux d'entrainement et de tests sont sous forme\n",
    "de listes contenants les mappings :(image prétraitée ---> jeton d'annotation(mot) )\n",
    "\"\"\"\n",
    "\n",
    "# Boucle pour construire le jeu d'entrainement\n",
    "img_name_train = []\n",
    "cap_train = []\n",
    "for imgt in img_name_train_keys:\n",
    "    capt_len = len(img_to_cap_vector[imgt])\n",
    "    # Duplication des images en le nombre d'annotations par image\n",
    "    img_name_train.extend([imgt] * capt_len)\n",
    "    cap_train.extend(img_to_cap_vector[imgt])\n",
    "\n",
    "# Boucle pour construire le jeu de test\n",
    "img_name_val = []\n",
    "cap_val = []\n",
    "for imgv in img_name_val_keys:\n",
    "    capv_len = len(img_to_cap_vector[imgv])\n",
    "    img_name_val.extend([imgv] * capv_len)\n",
    "    cap_val.extend(img_to_cap_vector[imgv])\n",
    "\n",
    "len(img_name_train), len(cap_train), len(img_name_val), len(cap_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.5. <a>Chargement des photos prétraitées</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfin, nous allons charger les photos qui ont été traitées en amont, en veillant bien à les mélanger et les répartir dans des batchs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N'hésitez pas à modifier ces paramètres en fonction de votre machine\n",
    "BATCH_SIZE = 64 # taille du batch\n",
    "BUFFER_SIZE = 1000 # taille du buffer pour melanger les donnes\n",
    "embedding_dim = 256\n",
    "units = 512 # Taille de la couche caché dans le RNN\n",
    "vocab_size = top_k + 1\n",
    "num_steps = len(img_name_train) // BATCH_SIZE\n",
    "\n",
    "# La forme du vecteur extrait à partir d'InceptionV3 est (64, 2048)\n",
    "# Les deux variables suivantes representent la forme de ce vecteur\n",
    "features_shape = 2048\n",
    "attention_features_shape = 64\n",
    "\n",
    "# Fonction qui charge les fichiers numpy des images prétraitées\n",
    "def map_func(img_name, cap):\n",
    "    img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
    "    return img_tensor, cap\n",
    "\n",
    "# Creation d'un dataset de \"Tensor\"s (sert à representer de grands dataset)\n",
    "# Le dataset est cree a partir de \"img_name_train\" et \"cap_train\"\n",
    "dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n",
    "\n",
    "# L'utilisation de map permet de charger les fichiers numpy (possiblement en parallèle)\n",
    "dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
    "          map_func, [item1, item2], [tf.float32, tf.int32]),\n",
    "          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Melanger les donnees et les diviser en batchs\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. <a id='model'>Création du modèle de captioning</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que les données ont été prétraitées et chargées correctement, nous pouvons procéder à la création du modèle qui va nous permettre d'automatiquement légender les photos issues des étapes précédentes du workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1. <a>Encodeur</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L’encodeur CNN est un module léger utilisé pour projeter les caractéristiques visuelles extraites d’une image (déjà pré-traitée par un réseau de type InceptionV3) dans un espace de plus petite dimension appelé espace d'embedding.  \n",
    " \n",
    "Ce module contient une seule couche Dense composée de `embedding_dim` neurones, suivie d’une fonction d’activation ReLU.  \n",
    "Son rôle est de réduire la dimension des descripteurs visuels (par ex. de `(64, 2048)` à `(64, 256)` si `embedding_dim = 256`), tout en conservant l'information nécessaire pour guider le décodeur dans la génération de la légende."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Détails des couches utilisées :**\n",
    "\n",
    "|     Couche     | Type  | Nombre de neurones |                                  Rôle                                        |\n",
    "|----------------|-------|--------------------|------------------------------------------------------------------------------|\n",
    "|      `fc`      | Dense |  `embedding_dim`   |               Réduction de dimensions des vecteurs d'image                   |\n",
    "|  `activation`  | ReLu  |      `units`       | Introduit de la non-linéarité pour permettre un apprentissage plus expressif |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.keras.utils.register_keras_serializable('CNN_Encoder')\n",
    "class CNN_Encoder(tf.keras.Model):\n",
    "    # Comme les images sont déjà prétraités par InceptionV3 est représenté sous forme compacte\n",
    "    # L'encodeur CNN ne fera que transmettre ces caractéristiques à une couche dense\n",
    "    def __init__(self, embedding_dim, **kwargs):\n",
    "        super(CNN_Encoder, self).__init__(**kwargs)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        # forme après fc == (batch_size, 64, embedding_dim)\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "    def call(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x\n",
    "    def get_config(self):\n",
    "        config = super(CNN_Encoder, self).get_config()\n",
    "        config.update({\n",
    "            'embedding_dim': self.embedding_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "──────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2. <a>Mécanisme de l'attention</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le mécanisme d’attention de Bahdanau permet au décodeur de se concentrer dynamiquement sur les régions les plus pertinentes d’une image à chaque étape de génération de la légende.\n",
    "\n",
    "Ce module calcule, pour chaque pas de temps, une pondération (poids d’attention) sur les vecteurs de caractéristiques extraits de l’image (par l’encodeur CNN), afin d’en dériver un vecteur de contexte. Ce vecteur est ensuite utilisé par le décodeur pour prédire le mot suivant de manière plus précise et contextualisée."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Détails des couches utilisées :**\n",
    "\n",
    "| Couche | Type  | Nombre de neurones |                               Rôle                                   |\n",
    "|--------|-------|--------------------|----------------------------------------------------------------------|\n",
    "|  `W1`  | Dense |      `units`       | Applique une projection linéaire aux features de l'image             |\n",
    "|  `W2`  | Dense |      `units`       | Projette l'état caché du décodeur pour le combiner avec les features |\n",
    "|  `V`   | Dense |         1          | Calcule un score d'attention pour chaque vecteur de feature          |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.keras.utils.register_keras_serializable('BahdanauAttention')\n",
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        # features(CNN_encoder output) forme == (batch_size, 64, embedding_dim)\n",
    "\n",
    "        # forme de la couche cachée == (batch_size, hidden_size)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "        attention_hidden_layer = tf.nn.tanh(\n",
    "                self.W1(features) + self.W2(hidden_with_time_axis)\n",
    "        )\n",
    "\n",
    "        # Cela vous donne un score non normalisé pour chaque caractéristique de l'image.\n",
    "        score = self.V(attention_hidden_layer)\n",
    "\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        \n",
    "        return context_vector, attention_weights\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(BahdanauAttention, self).get_config()\n",
    "        config.update({\n",
    "            units: self.units,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "──────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3. <a>Décodeur</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le décodeur RNN est un module séquentiel chargé de générer la légende de l’image mot par mot, en s’appuyant à chaque étape sur :\n",
    "\n",
    "- le mot précédent généré (transformé en vecteur via un embedding)\n",
    "- le vecteur de contexte issu du mécanisme d’attention\n",
    "- l’état caché précédent\n",
    "\n",
    "Ce décodeur existe sous deux variantes : une version basée sur un GRU (Gated Recurrent Unit), et une autre basée sur un LSTM (Long Short-Term Memory). Ces deux architectures permettent de modéliser des dépendances temporelles dans les séquences, tout en gérant l’oubli d’informations anciennes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Détails des couches utilisées :**\n",
    "\n",
    "|    Couche     |       Type        |      Nombre de neurones       |                               Rôle                                   |\n",
    "|---------------|-------------------|-------------------------------|----------------------------------------------------------------------|\n",
    "|  `embedding`  |     Embedding     | `(vocab_size, embedding_dim)` | Convertit les mots (entiers) en vecteurs continus             |\n",
    "|  `attention`  | BahdanauAttention |             `units`           | Calcule un vecteur de contexte pondéré à partir des features de l’image |\n",
    "|    `fc1`      |       Dense       |             `units`           | (GRU uniquement) Réduit la concaténation `[contexte + embedding]` à une dimension compatible avec la GRU          |\n",
    "|   `layer`     |    GRU ou LSTM    |             `units`           | Génère le nouvel état caché et une sortie intermédiaire          |\n",
    "|    `fc2`      |       Dense       |           `vocab_size`        | Prédit le mot suivant en sortie (logits sur le vocabulaire)          |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.keras.utils.register_keras_serializable('RNN_Decoder')\n",
    "class RNN_Decoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size, use_lstm=False, **kwargs):\n",
    "        super(RNN_Decoder, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.use_lstm = use_lstm\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        if not use_lstm:\n",
    "            self.layer = tf.keras.layers.GRU(\n",
    "                self.units,\n",
    "                return_sequences=True,\n",
    "                return_state=True,\n",
    "                activation='tanh',\n",
    "                recurrent_activation='sigmoid',\n",
    "                use_bias=True,\n",
    "                kernel_initializer='glorot_uniform',\n",
    "                recurrent_initializer='orthogonal',\n",
    "                bias_initializer='zeros',\n",
    "                unroll=True,\n",
    "            )\n",
    "        else:\n",
    "            self.layer = tf.keras.layers.LSTM(\n",
    "                self.units,\n",
    "                return_sequences=True,\n",
    "                return_state=True,\n",
    "                activation='tanh',\n",
    "                recurrent_activation='sigmoid',\n",
    "                use_bias=True,\n",
    "                kernel_initializer='glorot_uniform',\n",
    "                recurrent_initializer='orthogonal',\n",
    "                bias_initializer='zeros',\n",
    "                unroll=True,\n",
    "            )\n",
    "\n",
    "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "        self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "    def call(self, x, features, hidden):\n",
    "        context_vector, attention_weights = self.attention(features, hidden)\n",
    "        x = self.embedding(x)\n",
    "        context_vector = tf.expand_dims(context_vector, 1)\n",
    "        \n",
    "        x = tf.concat([context_vector, x], axis=-1)\n",
    "\n",
    "        if not self.use_lstm:\n",
    "            x = self.fc1(x)\n",
    "            output, state = self.layer(x)\n",
    "            y = tf.reshape(output, (-1, output.shape[2]))\n",
    "        else:\n",
    "            output, state, _ = self.layer(x)\n",
    "            y = self.fc1(output)\n",
    "            y = tf.reshape(y, (-1, y.shape[2]))\n",
    "\n",
    "        y = self.fc2(y)\n",
    "\n",
    "        return y, state, attention_weights\n",
    "\n",
    "    def reset_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units))\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(RNN_Decoder, self).get_config()\n",
    "        config.update({\n",
    "            'embedding_dim': self.embedding_dim,\n",
    "            'units': self.units,\n",
    "            'vocab_size': self.vocab_size,\n",
    "            'use_lstm': self.use_lstm\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 256\n",
    "units = 512\n",
    "vocab_size = top_k + 1\n",
    "\n",
    "# Création de l'encodeur (CNN)\n",
    "encoder = CNN_Encoder(embedding_dim)\n",
    "\n",
    "# Création du décodeur (RNN avec attention)\n",
    "decoderLSTM = RNN_Decoder(embedding_dim, units, vocab_size, use_lstm=True)\n",
    "decoderGRU = RNN_Decoder(embedding_dim, units, vocab_size, use_lstm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "──────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.4. <a>Optimizer</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L’optimiseur est l’algorithme chargé de mettre à jour les poids du modèle pendant l’apprentissage, en minimisant la fonction de perte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimiseur ADAM\n",
    "optimizer_LSTM = tf.keras.optimizers.Adam()\n",
    "optimizer_GRU = tf.keras.optimizers.Adam()\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    \n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. <a id='training'>Entraînement du modèle</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1. <a>Méthodes d'entraînement</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le code ci-dessous est une étape de l'entrainement où nous initialisons l'entrée du décodeur et calculons la loss ainsi que le gradient pour mettre à jour les poids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(img_tensor, target, encoder, decoder, optimizer):\n",
    "    loss = 0\n",
    "\n",
    "    # Initialisation de l'état caché pour chaque batch\n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "\n",
    "    # Initialiser l'entrée du décodeur\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
    "    \n",
    "    with tf.GradientTape() as tape: # Offre la possibilité de calculer le gradient du loss\n",
    "        features = encoder(img_tensor)\n",
    "\n",
    "        for i in range(1, target.shape[1]):\n",
    "            # Prédiction des i'èmes mot du batch avec le décodeur\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "            # Le mot correct à l'étap i est donné en entrée à l'étape (i+1)\n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "    total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "    \n",
    "    # Enregistrement de la loss pour chaque batch\n",
    "    return loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(encoder, decoder, optimizer, dataset, epochs = 50, early_stopping = None):\n",
    "    loss_plot = []\n",
    "    strike = 0\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    checkpoint_path = \"./checkpoints/model\"\n",
    "    ckpt = tf.train.Checkpoint(encoder=encoder, decoder=decoder, optimizer=optimizer)\n",
    "    ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=1)\n",
    "\n",
    "    # création des fichiers pour l'early stopping\n",
    "    if early_stopping != None and not os.path.exists(f\"{ProjectPath}/checkpoints/encoder.weights.h5\"):\n",
    "        Path(f'{ProjectPath}/checkpoints/encoder.weights.h5').touch()\n",
    "\n",
    "    if early_stopping != None and not os.path.exists(f\"{ProjectPath}/checkpoints/decoder.weights.h5\"):\n",
    "        Path(f'{ProjectPath}/checkpoints/decoder.weights.h5').touch()\n",
    "\n",
    "    #if ckpt_manager.latest_checkpoint:\n",
    "    #    ckpt.restore(ckpt_manager.latest_checkpoint).expect_partial()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        total_loss = 0\n",
    "\n",
    "        for (batch, (img_tensor, target)) in enumerate(dataset):\n",
    "            batch_loss, t_loss = train_step(img_tensor, target, encoder, decoder, optimizer)\n",
    "            total_loss += t_loss\n",
    "\n",
    "            #if batch % 50 == 0:\n",
    "                #print('Epoch {}, Batch {}, Batch Loss {:.4f}, Time taken {:.2f} mins'.format(epoch + 1, batch, batch_loss.numpy() / int(target.shape[1]), (time.time() - start)/60))\n",
    "\n",
    "        epoch_loss = total_loss / num_steps\n",
    "        loss_plot.append(epoch_loss)\n",
    "\n",
    "        # Sauvegarde du modèle uniquement si la perte est meilleure\n",
    "        if float('%.2f' % (epoch_loss)) < best_loss: # Si vous trouver une meilleur façon pour arrondir le float, n'hésitez pas à le changer\n",
    "            strike = 0\n",
    "            best_loss = float('%.2f' % (epoch_loss))\n",
    "            encoder.save_weights(f\"{ProjectPath}/checkpoints/encoder.weights.h5\")\n",
    "            decoder.save_weights(f\"{ProjectPath}/checkpoints/decoder.weights.h5\")\n",
    "            ckpt_save_path = ckpt_manager.save()\n",
    "        else:\n",
    "            strike += 1\n",
    "            if early_stopping != None and strike >= early_stopping:\n",
    "                encoder.load_weights(f\"{ProjectPath}/checkpoints/encoder.weights.h5\")\n",
    "                decoder.load_weights(f\"{ProjectPath}/checkpoints/decoder.weights.h5\")\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "        print('Result Epoch {}, Best Loss {}, Epoch Loss {}, Time taken {:.2f} mins, strike {}'.format(epoch + 1, best_loss, float('%.2f' % (epoch_loss)), (time.time() - start)/60, strike))\n",
    "    \n",
    "    return [loss_plot, best_loss, encoder, decoder, optimizer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "──────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2. <a>Entraînement</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = train_model(encoder, decoderLSTM, optimizer_LSTM, dataset, epochs = 1, early_stopping = 10)\n",
    "\n",
    "# Affichage de la courbe d'entrainement\n",
    "plt.plot(results[0])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.show()\n",
    "\n",
    "#results[2].save('models/encoder_model.keras')  \n",
    "#results[3].save('models/decoder_LSTM_model.keras')\n",
    "\n",
    "results[2].save_weights(\"models/encoder.weights.h5\")\n",
    "results[3].save_weights(\"models/decoder_lstm.weights.h5\")\n",
    "\n",
    "results = train_model(encoder, decoderGRU, optimizer_GRU, dataset, epochs = 1, early_stopping = 10)\n",
    "\n",
    "## Affichage de la courbe d'entrainement\n",
    "plt.plot(results[0])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.show()\n",
    "\n",
    "#results[3].save('models/decoder_GRU_model.keras')\n",
    "results[3].save_weights(\"models/decoder_gru.weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style=\"color:yellow;\">TODO</b>\n",
    "\n",
    "Afficher le graphique et analyser les résultats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "──────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.3. <a>Sauvegarde</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le code suivant permet de sauvegarder les résultats obtenus sous la forme d'un fichier `.pickle`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# saving\n",
    "if not os.path.exists(f'{ProjectPath}/models/tokenizer.pickle'):\n",
    "    Path(f'{ProjectPath}/models/tokenizer.pickle').touch()\n",
    "with open(f'{ProjectPath}/models/tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. <a id=\"evaluation\">Évaluation du modèle</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.1. <a>Chargement des poids</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading\n",
    "with open(f'{ProjectPath}/models/tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu dois d'abord reconstruire les modèles (même structure qu'au training)\n",
    "encoder_load = CNN_Encoder(embedding_dim)\n",
    "decoder_load_lstm = RNN_Decoder(embedding_dim, units, vocab_size, use_lstm=True)\n",
    "decoder_load_gru = RNN_Decoder(embedding_dim, units, vocab_size, use_lstm=False)\n",
    "\n",
    "\n",
    "# Build the models by calling them with dummy data\n",
    "dummy_image_features = tf.random.normal([1, attention_features_shape, features_shape])\n",
    "dummy_caption_input = tf.random.uniform([1, 1], minval=0, maxval=vocab_size, dtype=tf.int32)\n",
    "dummy_hidden_state = tf.zeros((1, units))\n",
    "\n",
    "# Build the encoder by calling it with dummy data\n",
    "_ = encoder_load(dummy_image_features)\n",
    "\n",
    "# Build the decoder by calling it with dummy data\n",
    "dummy_encoder_output = encoder_load(dummy_image_features)\n",
    "_ = decoder_load_lstm(dummy_caption_input, dummy_encoder_output, dummy_hidden_state)\n",
    "_ = decoder_load_gru(dummy_caption_input, dummy_encoder_output, dummy_hidden_state)\n",
    "\n",
    "\n",
    "# Puis charger les poids\n",
    "encoder_load.load_weights(\"models/encoder.weights.h5\")\n",
    "decoder_load_lstm.load_weights(\"models/decoder_lstm.weights.h5\")\n",
    "decoder_load_gru.load_weights(\"models/decoder_gru.weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "──────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.2. <a>Méthode d'affichage de l'attention</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display attention on the image\n",
    "def plot_attention(image, result, attention_plot):\n",
    "    temp_image = np.array(Image.open(image))\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "    len_result = len(result)\n",
    "    for l in range(len_result):\n",
    "        temp_att = np.resize(attention_plot[l], (8, 8))\n",
    "        ax = fig.add_subplot(len_result // 2, len_result // 2, l + 1)\n",
    "        ax.set_title(result[l])\n",
    "        img = ax.imshow(temp_image)\n",
    "        ax.imshow(temp_att, cmap='gray', \n",
    "        alpha=0.6, extent=img.get_extent())\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "──────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.3. <a>Évaluation</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(image, decoder):\n",
    "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
    "\n",
    "    hidden = decoder.reset_state(batch_size=1)\n",
    "\n",
    "    temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
    "    img_tensor_val = image_features_extract_model(temp_input)\n",
    "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "\n",
    "    features = encoder_load(img_tensor_val)\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
    "    result = []\n",
    "\n",
    "    for i in range(max_length):\n",
    "        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
    "\n",
    "        attention_plot[i] = tf.reshape(attention_weights, (-1,)).numpy()\n",
    "\n",
    "        #temperature = 0.7  \n",
    "        #predictions = predictions / temperature\n",
    "        \n",
    "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
    "        \n",
    "        result.append(tokenizer.index_word[predicted_id])\n",
    "    \n",
    "        if tokenizer.index_word[predicted_id] == '<end>':\n",
    "            break\n",
    "        \n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    attention_plot = attention_plot[:len(result), :]\n",
    "    return result, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour valider une légende\n",
    "def is_valid_caption(caption):\n",
    "    words = caption.split()\n",
    "\n",
    "    # Critère 1 : au moins 5 mots\n",
    "    if len(words) < 5:\n",
    "        return False\n",
    "\n",
    "    # Critère 2 : au moins 3 mots de au moins 5 lettres\n",
    "    long_words = [w for w in words if len(w) >= 5]\n",
    "    if len(long_words) < 3:\n",
    "        return False\n",
    "\n",
    "    # Critère 3 : pas plus de 2 occurrences du même mot\n",
    "    lower_words = [w.lower() for w in words]\n",
    "    word_counts = collections.Counter(lower_words)\n",
    "    if any(count > 2 for count in word_counts.values()):\n",
    "        return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import glob\n",
    "#image_folder = \"dataset_livrable_2/Results/denoised_photos/\"\n",
    "#image_files = glob.glob(f\"{image_folder}/*.jpg\")\n",
    "#image = random.choice(image_files)\n",
    "#index = None\n",
    "\n",
    "#Choisir une image aléatoire\n",
    "rid = np.random.randint(0, len(img_name_val))\n",
    "image = img_name_val[rid]\n",
    "index = img_name_val.index(image)\n",
    "real_caption = ' '.join([\n",
    "    tokenizer.index_word[i]\n",
    "    for i in cap_val[index]\n",
    "    if i not in [0, tokenizer.word_index['<start>'], tokenizer.word_index['<end>']]\n",
    "])\n",
    "\n",
    "valid_captions_LSTM = []\n",
    "valid_captions_GRU = []\n",
    "\n",
    "# Boucle jusqu'à obtenir 5 captions valides par modèle\n",
    "while len(valid_captions_LSTM) < 5 or len(valid_captions_GRU) < 5:\n",
    "    result_LSTM, attention_plot_LSTM = evaluate(image, decoder_load_lstm)\n",
    "    result_GRU, attention_plot_GRU = evaluate(image, decoder_load_gru)\n",
    "\n",
    "    predicted_caption_LSTM = ' '.join([word for word in result_LSTM if word not in ['<start>', '<end>']])\n",
    "    predicted_caption_GRU = ' '.join([word for word in result_GRU if word not in ['<start>', '<end>']])\n",
    "\n",
    "    if len(valid_captions_LSTM) < 5 and is_valid_caption(predicted_caption_LSTM):\n",
    "        valid_captions_LSTM.append((predicted_caption_LSTM, attention_plot_LSTM))\n",
    "\n",
    "    if len(valid_captions_GRU) < 5 and is_valid_caption(predicted_caption_GRU):\n",
    "        valid_captions_GRU.append((predicted_caption_GRU, attention_plot_GRU))\n",
    "\n",
    "# Affichage final\n",
    "print('Image Path:', image)\n",
    "print('Real Caption:', real_caption)\n",
    "\n",
    "# Affichage LSTM\n",
    "print('LSTM Predictions:')\n",
    "for i, (caption, attn) in enumerate(valid_captions_LSTM, 1):\n",
    "    print(f'LSTM Prediction {i}:', caption)\n",
    "    #plot_attention(image, caption.split(), attn)\n",
    "\n",
    "# Affichage GRU\n",
    "print('GRU Predictions:')\n",
    "for i, (caption, attn) in enumerate(valid_captions_GRU, 1):\n",
    "    print(f'GRU Prediction {i}:', caption)\n",
    "    #plot_attention(image, caption.split(), attn)\n",
    "\n",
    "# Afficher l'image originale\n",
    "img = Image.open(image)\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Image utilisée\")\n",
    "plt.show()\n",
    "\n",
    "# Metrics\n",
    "#print('Precision score {}'.format(real_caption.split() - caption[0].split()))\n",
    "#print('Recall score {}'.format(caption[0].split() - real_caption.split()))\n",
    "#print('BLEU score -> {}'.format(sentence_bleu(real_caption.split(), caption[0].split())))\n",
    "#Meteor TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style=\"color:yellow;\">TODO</b>\n",
    "\n",
    "Afficher quelques images et analyser les résultats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "──────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.4. <a>Évaluation d'un modèle entraîné sur 50 epochs</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette section va permettre de charger les poids et de tester un modèle qui a été entraîné sur 50 epochs au total, donnant donc naturellement de meilleurs résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_load_lstm = RNN_Decoder(embedding_dim, units, vocab_size, use_lstm=True)\n",
    "\n",
    "dummy_image_features = tf.random.normal([1, attention_features_shape, features_shape])\n",
    "dummy_caption_input = tf.random.uniform([1, 1], minval=0, maxval=vocab_size, dtype=tf.int32)\n",
    "dummy_hidden_state = tf.zeros((1, units))\n",
    "\n",
    "# Build the encoder by calling it with dummy data\n",
    "_ = encoder_load(dummy_image_features)\n",
    "\n",
    "# Build the decoder by calling it with dummy data\n",
    "dummy_encoder_output = encoder_load(dummy_image_features)\n",
    "_ = decoder_load_lstm(dummy_caption_input, dummy_encoder_output, dummy_hidden_state)\n",
    "_ = decoder_load_gru(dummy_caption_input, dummy_encoder_output, dummy_hidden_state)\n",
    "\n",
    "\n",
    "# Puis charger les poids\n",
    "decoder_load_lstm.load_weights(\"models/decoder_lstm_better.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import glob\n",
    "#image_folder = \"dataset_livrable_2/Results/denoised_photos/\"\n",
    "#image_files = glob.glob(f\"{image_folder}/*.jpg\")\n",
    "#image = random.choice(image_files)\n",
    "#index = None\n",
    "\n",
    "#Choisir une image aléatoire\n",
    "rid = np.random.randint(0, len(img_name_val))\n",
    "image = img_name_val[rid]\n",
    "index = img_name_val.index(image)\n",
    "real_caption = ' '.join([\n",
    "    tokenizer.index_word[i]\n",
    "    for i in cap_val[index]\n",
    "    if i not in [0, tokenizer.word_index['<start>'], tokenizer.word_index['<end>']]\n",
    "])\n",
    "\n",
    "valid_captions_LSTM = []\n",
    "valid_captions_GRU = []\n",
    "\n",
    "# Boucle jusqu'à obtenir 5 captions valides par modèle\n",
    "while len(valid_captions_LSTM) < 5 or len(valid_captions_GRU) < 5:\n",
    "    result_LSTM, attention_plot_LSTM = evaluate(image, decoder_load_lstm)\n",
    "\n",
    "    predicted_caption_LSTM = ' '.join([word for word in result_LSTM if word not in ['<start>', '<end>']])\n",
    "\n",
    "    if len(valid_captions_LSTM) < 5 and is_valid_caption(predicted_caption_LSTM):\n",
    "        valid_captions_LSTM.append((predicted_caption_LSTM, attention_plot_LSTM))\n",
    "\n",
    "\n",
    "# Affichage final\n",
    "print('Image Path:', image)\n",
    "print('Real Caption:', real_caption)\n",
    "\n",
    "# Affichage LSTM\n",
    "print('LSTM Predictions:')\n",
    "for i, (caption, attn) in enumerate(valid_captions_LSTM, 1):\n",
    "    print(f'LSTM Prediction {i}:', caption)\n",
    "    #plot_attention(image, caption.split(), attn)\n",
    "\n",
    "# Afficher l'image originale\n",
    "img = Image.open(image)\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Image utilisée\")\n",
    "plt.show()\n",
    "\n",
    "# Metrics\n",
    "#print('Precision score {}'.format(real_caption.split() - caption[0].split()))\n",
    "#print('Recall score {}'.format(caption[0].split() - real_caption.split()))\n",
    "#print('BLEU score -> {}'.format(sentence_bleu(real_caption.split(), caption[0].split())))\n",
    "#Meteor TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style=\"color:yellow;\">TODO</b>\n",
    "\n",
    "Afficher quelques images et analyser les résultats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. <a id=\"metrics\">Calcul des métriques</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour évaluer la qualité des légendes générées par le modèle, plusieurs métriques de similarité de texte couramment utilisées en traitement automatique du langage naturel (TALN) ont été sélectionnées. Ces métriques permettent de quantifier la proximité linguistique et sémantique entre les légendes prédites et les légendes de référence associées aux images.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "**BLEU (Bilingual Evaluation Understudy)**\n",
    "BLEU est une métrique fondée sur le taux de recouvrement de n-grammes entre la prédiction et les références. Elle est historiquement utilisée pour l’évaluation de la traduction automatique, mais s’applique également au captioning.  \n",
    "Elle est adaptée pour mesurer la précision lexicale, mais tend à pénaliser les reformulations sémantiques correctes mais différentes des références.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "**METEOR (Metric for Evaluation of Translation with Explicit ORdering)**\n",
    "METEOR complète les limites du BLEU en tenant compte de la racine des mots (stemming), des synonymes et de l’ordre des mots, ce qui la rend plus sensible à la variation sémantique.  \n",
    "Elle est donc plus corrélée avec les jugements humains pour des phrases courtes et variées, comme celles produites en image captioning.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "**ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**\n",
    "ROUGE, souvent utilisée en résumé automatique, mesure principalement le recouvrement de sous-séquences (n-grammes, longest common subsequence, etc.).  \n",
    "Elle est utile pour évaluer la couverture du contenu informatif de la prédiction par rapport à la référence.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "**CIDEr (Consensus-based Image Description Evaluation)**\n",
    "CIDEr est une métrique spécifiquement conçue pour l’évaluation de la génération de légendes d’images. Elle mesure la similarité entre les mots utilisés dans la légende générée et ceux des références, en tenant compte de la fréquence inverse des documents (TF-IDF).  \n",
    "Elle privilégie donc les expressions rares et distinctives, souvent importantes pour capturer l’unicité d’une image. CIDEr est reconnu pour mieux refléter la qualité perçue par l’humain dans le cadre du captioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate as evaluate_metric\n",
    "import nltk\n",
    "\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "from pycocoevalcap.spice.spice import Spice\n",
    "from pycocoevalcap.tokenizer.ptbtokenizer import PTBTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "bleus = []\n",
    "rouges = []\n",
    "meteors = []\n",
    "CIDErs= []\n",
    "\n",
    "bleu = evaluate_metric.load(\"bleu\", verbose=False)\n",
    "meteor = evaluate_metric.load(\"meteor\", verbose=False)\n",
    "rouge = evaluate_metric.load(\"rouge\", verbose=False)\n",
    "cider_scorer = Cider()\n",
    "\n",
    "for i, (caption, attn) in enumerate(valid_captions_LSTM, 1):\n",
    "    predictions = [caption]\n",
    "    references = [[real_caption]]\n",
    "    print(predictions)\n",
    "    \n",
    "    # BLEU       \n",
    "    bleus.append(bleu.compute(predictions=predictions, references=references)[\"bleu\"])\n",
    "\n",
    "    # METEOR\n",
    "    meteors.append(meteor.compute(predictions=predictions, references=references)[\"meteor\"])\n",
    "\n",
    "    # ROUGE\n",
    "    rouges.append(rouge.compute(predictions=predictions, references=references)[\"rougeLsum\"])\n",
    "\n",
    "    gts = {\n",
    "        \"img1\": [{\"caption\": real_caption}],\n",
    "    }\n",
    "    res = {\n",
    "        \"img1\": [{\"caption\": caption[0]}],\n",
    "    }\n",
    "\n",
    "    # CIDErs\n",
    "    PTBtokenizer = PTBTokenizer()\n",
    "    gts = PTBtokenizer.tokenize(gts)\n",
    "    res = PTBtokenizer.tokenize(res)\n",
    "\n",
    "    score, scores = cider_scorer.compute_score(gts, res)\n",
    "    CIDErs.append(score)\n",
    "\n",
    "# SPICE\n",
    "# spice_scorer = Spice()\n",
    "# score, scores = spice_scorer.compute_score(gts, res)\n",
    "# print(f\"SPICE: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"caption \" + str(i) for i in range(len(bleus))]\n",
    "plt.bar(labels, bleus, color='blue', label='BLEU')\n",
    "plt.bar(labels, rouges, color='red', label='ROUGE')\n",
    "plt.bar(labels, meteors, color='green', label='METEOR')\n",
    "plt.bar(labels, CIDErs, color='orange', label='CIDEr')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. <a id=\"conclusion\">Conclusion</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce livrable nous as permis de concevoir et d’évaluer un système de génération automatique de légendes d’images basé sur une architecture encodeur-décodeur avec mécanisme d’attention. L’encodeur CNN, associé à une attention de type Bahdanau, a efficacement extrait et mis en valeur les caractéristiques visuelles pertinentes. Le décodeur, implémenté en deux variantes (GRU et LSTM), a généré des séquences de texte cohérentes et adaptées aux images.\n",
    "\n",
    "L’utilisation de l’optimiseur Adam a favorisé un apprentissage stable, et l’évaluation selon les métriques BLEU, METEOR, ROUGE et CIDEr a permis de mesurer avec précision la qualité des légendes produites. Les résultats confirment la pertinence de l’approche retenue pour la tâche de captioning d’images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
