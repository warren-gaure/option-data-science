{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> **Livrable n°1 : Classification binaire** </center>\n",
    "\n",
    "‎ \n",
    "\n",
    "Réalisé par le **groupe n°X** :\n",
    "- GAURE Warren\n",
    "- Membre n°2\n",
    "- Membre n°3\n",
    "- Membre n°4\n",
    "\n",
    "‎\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Sommaire**\n",
    "\n",
    "1. [Mise en contexte](#contexte)\n",
    "2. [Objectif du livrable](#objectif)\n",
    "3. [Importation des bibliothèques](#import)\n",
    "4. [Chargement et préparation des données](#load)\n",
    "5. [Exploration et visualisation des données](#exploration)\n",
    "6. [Configuration de l'environnement](#configuration)\n",
    "7. [Choix de l'architecture](#architecture)\n",
    "8. [Réalisation du modèle](#modele)\n",
    "9. [Entraînement et évaluation du modèle](#train)\n",
    "10. [Amélioration du modèle](#amelioration)\n",
    "11. [Modèle final](#final)\n",
    "12. [Conclusion](#conclusion)\n",
    "\n",
    "‎ \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. <a id='contexte'>Mise en contexte</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L’entreprise TouNum est spécialisée dans la numérisation de documents, qu’il s’agisse de textes ou d’images. Ses services sont particulièrement sollicités par des entreprises cherchant à transformer leur base documentaire papier en fichiers numériques exploitables. Aujourd’hui, TouNum souhaite aller plus loin en enrichissant son offre avec des outils basés sur le Machine Learning.\n",
    "\n",
    "En effet, certains clients disposent d’un volume considérable de documents à numériser et expriment un besoin croissant pour des solutions de catégorisation automatique. Une telle innovation leur permettrait d’optimiser le traitement et l’exploitation de leurs données numérisées. Toutefois, TouNum ne dispose pas en interne des compétences nécessaires pour concevoir et mettre en place ces technologies.\n",
    "\n",
    "C’est dans ce cadre que notre équipe de spécialistes en Data Science du CESI est sollicitée. Notre mission consiste à développer une première solution intégrant du captioning automatique : un système capable d’analyser des photographies et de générer une légende descriptive de manière autonome.\n",
    "\n",
    "Heureusement, TouNum possède déjà plusieurs milliers d’images annotées, ce qui constituera une ressource précieuse pour entraîner les modèles de Machine Learning à partir d’un apprentissage supervisé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. <a id='objectif'>Objectif du livrable</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TouNum souhaite automatiser la sélection des photos destinées à l'annotation. Ce livrable propose une méthode de classification basée sur les réseaux de neurones pour filtrer les images qui ne sont pas des photos. La solution reposera sur l'architecture de réseau retenue en fonction des résultats obtenus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. <a id='import'>Importation des bibliothèques</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "import pathlib\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. <a id='load'>Chargement et préparation des données</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant charger et préparer les données fournies par TouNum.\n",
    "\n",
    "Les images doivent être séparées en deux ensembles : l'un pour entraîner le modèle, l'autre pour l'évaluer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = './dataset_livrable_1'\n",
    "image_h = 180\n",
    "image_w = 180\n",
    "batch_s = 32\n",
    "\n",
    "train_set = keras.utils.image_dataset_from_directory(\n",
    "    dataset_path,\n",
    "    label_mode = \"int\",\n",
    "    batch_size = batch_s,\n",
    "    image_size = (image_h, image_w),\n",
    "    seed = 42,\n",
    "    validation_split = 0.2,\n",
    "    subset = \"training\"\n",
    ")\n",
    "\n",
    "test_set = keras.utils.image_dataset_from_directory(\n",
    "    dataset_path,\n",
    "    label_mode = \"int\",\n",
    "    batch_size = batch_s,\n",
    "    image_size = (image_h, image_w),\n",
    "    seed = 42,\n",
    "    validation_split = 0.2,\n",
    "    subset = \"validation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La sortie indique la présence de **XXXX** fichiers au total, dont **XXXX** appartenant au jeu d'entraînement et **XXXX** au jeu de test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois les données réparties dans les deux ensembles, nous pouvons afficher le nom des labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train_set.class_names\n",
    "print(f\"Classes détectées : {class_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre jeu de données contient 5 catégories, mais nous cherchons simplement à distinguer les photos des autres images. Nous devons donc modifier les labels pour reformuler le problème en classification *binaire*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photo_index = class_names.index('Photo')\n",
    "\n",
    "def binary_conversion(image, label):\n",
    "    return image, tf.cast(tf.equal(label, photo_index), tf.int32)\n",
    "\n",
    "train_set = train_set.map(binary_conversion)\n",
    "test_set = test_set.map(binary_conversion)\n",
    "\n",
    "for image, label in train_set.take(1):\n",
    "    print(f\"Labels après conversion : {label.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme observé dans la sortie, la conversion est réussie et nous avons bien uniquement deux labels, `1` pour les photos et `0` pour les images qui ne sont pas des photos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après conversion des labels, nous devons nous assurer de la répartition équitable des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_label_distribution(dataset):\n",
    "    nb_photos = sum(1 for _, label in dataset.unbatch() if label.numpy() == 1)\n",
    "    nb_non_photos = sum(1 for _, label in dataset.unbatch() if label.numpy() == 0)\n",
    "    total = nb_photos + nb_non_photos\n",
    "\n",
    "    print(f\"Nombre total d'images : {total}\")\n",
    "    print(f\"Nombre de photos : {nb_photos} ({nb_photos/total*100:.2f}% du total)\")\n",
    "    print(f\"Nombre de 'non-photos' : {nb_non_photos} ({nb_non_photos/total*100:.2f}% du total)\")\n",
    "\n",
    "print_label_distribution(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La répartition des données est déséquilibrée, avec une majorité d'images non-photographiques. Pour corriger cela, nous allons utiliser la [Data Augmentation](https://en.wikipedia.org/wiki/Data_augmentation).\n",
    "\n",
    "Il s'agit d'une technique permettant de générer artificiellement de nouvelles images en appliquant des transformations aléatoires aux données existantes. Elle permet d'améliorer la généralisation du modèle en le rendant plus robuste aux variations comme l’orientation, la luminosité ou le zoom. Dans ce projet, la Data Augmentation est appliquée uniquement aux images de la classe minoritaire afin de rééquilibrer le dataset. Ainsi, le modèle évite d’apprendre un biais dû à un déséquilibre entre les classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = [\n",
    "    layers.RandomFlip(input_shape=(image_h, image_w, 3), mode='horizontal_and_vertical'),\n",
    "    layers.RandomRotation(factor=0.1, fill_mode='nearest'),\n",
    "    layers.RandomZoom(height_factor=0.1, fill_mode='nearest')\n",
    "]\n",
    "\n",
    "def augment_photos(image, label):\n",
    "    if tf.equal(label, 1):\n",
    "        image = data_augmentation(image)\n",
    "    return image, label\n",
    "\n",
    "train_set = train_set.map(augment_photos)\n",
    "\n",
    "print_label_distribution(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les données sont bien plus équilibrées dès à présent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. <a id='exploration'>Exploration et visualisation des données</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois les données chargées et les labels modifiés, nous pouvons examiner plus en détail les images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous affichons quelques images pour regarder plus en détail ce à quoi nous avons affaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "for images, labels in train_set.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(labels[i].numpy())\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfin, nous affichons la taille des données, information pouvant être utile pour gérer les performances de notre modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(train_set.take(1)))\n",
    "print(f\"Tensor des images : {images.shape}\")\n",
    "print(f\"Tensor des labels : {labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. <a id='configuration'>Configuration de l'environnement</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour optimiser les performances des calculs, nous allons configurer les données à l’aide de deux fonctions : `Dataset.cache` et `Dataset.prefetch`.  \n",
    "- [`Dataset.cache`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#cache) stocke les données en mémoire pour éviter les accès répétés au disque.  \n",
    "- [`Dataset.prefetch`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#prefetch) permet de traiter un élément en arrière-plan pendant l'entraînement ou l'évaluation.  \n",
    "\n",
    "En combinant ces techniques, nous réduirons significativement le temps de traitement et la charge computationnelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "train_set = train_set.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "test_set = test_set.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. <a id='architecture'>Choix de l'architecture</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons choisi une architecture basée sur les **réseaux de neurones convolutifs (CNN)**, particulièrement adaptés à la classification binaire d’images. Contrairement aux réseaux classiques (MLP), qui traitent chaque pixel indépendamment, les CNN exploitent les relations entre pixels pour mieux détecter les motifs et les formes. Grâce à leurs **filtres partagés**, ils réduisent considérablement le nombre de paramètres, rendant l’entraînement plus rapide et limitant le surapprentissage. Ces propriétés en font une solution puissante et efficace pour notre problème."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. <a id='modele'>Réalisation du modèle</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que le choix de l'architecture est fait, nous pouvons commencer à créer le modèle que nous allons utiliser pour classifier les images envoyées par l'entreprise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre modèle sera structuré autour des blocs suivants :  \n",
    "- Une **couche de rescaling** pour normaliser les valeurs des composantes RGB des pixels dans l'intervalle `[0;1]`.  \n",
    "- Une **première convolution** avec 16 filtres de taille 3x3 (`Conv2D`), suivie d'un **max pooling** pour réduire la dimension spatiale.  \n",
    "- Une **seconde convolution** utilisant 32 filtres de taille 3x3.  \n",
    "- Une **troisième convolution** avec 64 filtres de taille 3x3.  \n",
    "- Une **transformation en vecteur** via une opération d'aplatissement (`Flatten`).  \n",
    "- Une **couche dense** de 128 unités pour capturer les caractéristiques extraites.  \n",
    "- Enfin, une **sortie entièrement connectée** avec 1 unité, correspondant à la classe cible.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 1\n",
    "\n",
    "model = Sequential([\n",
    "    layers.Rescaling(1./255),\n",
    "    layers.Conv2D(16, (3, 3), padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(32, (3, 3), padding='same', activation='relu'),\n",
    "    layers.Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(num_classes, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'optimiseur [`Adam`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam) est choisi pour sa capacité d'adaptation et sa rapidité de convergence, combinant les avantages de [`Momentum`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/MomentumOptimizer) et [`RMSprop`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/RMSprop).\n",
    "\n",
    "La fonction de perte [`BinaryCrossentropy`](https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy) est utilisée car elle est plus efficace en mémoire et bien adaptée à la classification binaire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le code suivant va permettre d'avoir un résumé du modèle tel qu'il est à ce stade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. <a id='train'>Entraînement et évaluation du modèle</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec le modèle créé, nous pouvons désormais procéder à son entraînement et son évaluation avec les ensembles de données à notre disposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "history = model.fit(\n",
    "    train_set,\n",
    "    validation_data = test_set,\n",
    "    epochs = epochs\n",
    ")\n",
    "\n",
    "accuracy = history.history['accuracy']\n",
    "validation_accuracy = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "validation_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, accuracy, label='Training Accuracy')\n",
    "plt.plot(epochs_range, validation_accuracy, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, validation_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style=\"color:yellow;\">TODO</b>\n",
    "\n",
    "Analyser les résultats et parler du sur-apprentissage ainsi que de la potentielle instabilité."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. <a id='amelioration'>Amélioration du modèle</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin de palier au surapprentissage observé et d’améliorer la généralisation du modèle, plusieurs techniques de régularisation ont été retenues :\n",
    "\n",
    "- [**Dropout**](#dropout) : Cette méthode consiste à désactiver aléatoirement un certain pourcentage de neurones à chaque itération lors de l'entraînement. Cela empêche le modèle de devenir trop dépendant de certaines connexions et encourage l'apprentissage de représentations plus robustes. Une valeur typique se situe entre 0.2 et 0.5 selon la complexité du réseau.\n",
    "\n",
    "- [**Early-Stopping**](#early-stopping) : Cette technique permet d'arrêter automatiquement l'entraînement lorsque la performance sur l’ensemble de validation commence à se dégrader. Elle évite d’entraîner le modèle trop longtemps, ce qui pourrait mener à un surajustement aux données d’entraînement. Un paramètre clé est la `patience`, qui définit le nombre d’époques d'attente avant d'interrompre l'entraînement si aucune amélioration n'est observée.\n",
    "\n",
    "- [**Régularisation L1 (Lasso)**](#l1) : L1 applique une pénalité sur la somme des valeurs absolues des poids du modèle. Cela pousse certains poids à devenir exactement nuls, ce qui permet une sparsité dans les connexions et peut aider à réduire l'impact des variables les moins importantes. Elle est particulièrement utile pour éliminer les caractéristiques inutiles dans des modèles fortement paramétrés.\n",
    "\n",
    "- [**Régularisation L2 (Ridge)**](#l2) : L2 impose une pénalité sur la somme des carrés des poids, ce qui réduit la magnitude des paramètres sans les annuler complètement. Cela permet de contrôler des valeurs extrêmes qui pourraient rendre le modèle trop sensible aux fluctuations des données d’entraînement. Cette méthode est couramment utilisée dans les réseaux convolutifs, où il est essentiel d'éviter des poids trop dominants.\n",
    "\n",
    "En testant et, potentiellement, combinant ces différentes approches, nous parviendrons à obtenir un modèle plus stable, robuste, et capable de mieux généraliser sur des données non vues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.1. <a id='dropout'>Dropout</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour cette approche, nous ajoutons un Dropout de 25% après le MaxPooling et un Dropout de 50% après la couche Dense, en se basant sur l’approche de [Keras pour le dataset MNIST](https://github.com/keras-team/keras/blob/keras-2/examples/mnist_cnn.py). Le premier limite la dépendance aux caractéristiques extraites, tandis que le second réduit le surajustement dans la partie dense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_dropout = Sequential([\n",
    "    layers.Rescaling(1./255),\n",
    "    layers.Conv2D(16, (3, 3), padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Conv2D(32, (3, 3), padding='same', activation='relu'),\n",
    "    layers.Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_classes, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_with_dropout.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "model_with_dropout.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons procéder à l'entraînement de cette version du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "history = model_with_dropout.fit(\n",
    "    train_set,\n",
    "    validation_data = test_set,\n",
    "    epochs = epochs\n",
    ")\n",
    "\n",
    "accuracy = history.history['accuracy']\n",
    "validation_accuracy = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "validation_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, accuracy, label='Training Accuracy')\n",
    "plt.plot(epochs_range, validation_accuracy, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, validation_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style=\"color:yellow;\">TODO</b>\n",
    "\n",
    "Analyser les résultats obtenus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "──────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.2. <a id='early-stopping'>Early Stopping</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour cette approche, nous ajoutons un Early Stopping basé sur la perte de validation, avec une patience de 5 époques, afin d’interrompre l’entraînement dès que le modèle cesse de s’améliorer et de conserver les meilleurs poids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "model_with_early_stopping = Sequential([\n",
    "    layers.Rescaling(1./255),\n",
    "    layers.Conv2D(16, (3, 3), padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(32, (3, 3), padding='same', activation='relu'),\n",
    "    layers.Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(num_classes, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_with_early_stopping.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "model_with_early_stopping.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons procéder à l'entraînement de cette version du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "history = model_with_early_stopping.fit(\n",
    "    train_set,\n",
    "    validation_data = test_set,\n",
    "    epochs = epochs,\n",
    "    callbacks = [early_stopping]\n",
    ")\n",
    "\n",
    "accuracy = history.history['accuracy']\n",
    "validation_accuracy = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "validation_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, accuracy, label='Training Accuracy')\n",
    "plt.plot(epochs_range, validation_accuracy, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, validation_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style=\"color:yellow;\">TODO</b>\n",
    "\n",
    "Analyser les résultats obtenus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "──────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.3. <a id='l1'>Régularisation L1 (Lasso)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons appliquer une régularisation L1 pour limiter la complexité du modèle en forçant certains poids à devenir exactement nuls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_l1 = Sequential([\n",
    "    layers.Rescaling(1./255),\n",
    "    layers.Conv2D(16, (3, 3), padding='same', activation='relu', kernel_regularizer=keras.regularizers.l1(0.01)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(32, (3, 3), padding='same', activation='relu', kernel_regularizer=keras.regularizers.l1(0.01)),\n",
    "    layers.Conv2D(64, (3, 3), padding='same', activation='relu', kernel_regularizer=keras.regularizers.l1(0.01)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu', kernel_regularizer=keras.regularizers.l1(0.01)),\n",
    "    layers.Dense(num_classes, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_with_l1.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "model_with_l1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons procéder à l'entraînement de cette version du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "history = model_with_l1.fit(\n",
    "    train_set,\n",
    "    validation_data = test_set,\n",
    "    epochs = epochs\n",
    ")\n",
    "\n",
    "accuracy = history.history['accuracy']\n",
    "validation_accuracy = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "validation_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, accuracy, label='Training Accuracy')\n",
    "plt.plot(epochs_range, validation_accuracy, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, validation_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style=\"color:yellow;\">TODO</b>\n",
    "\n",
    "Analyser les résultats obtenus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "──────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.4. <a id='l2'>Régularisation L2 (Ridge)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant appliquer une régularisation L2 (Ridge) afin de réduire la magnitude des poids sans les annuler complètement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_l2 = Sequential([\n",
    "    layers.Rescaling(1./255),\n",
    "    layers.Conv2D(16, (3, 3), padding='same', activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(32, (3, 3), padding='same', activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    layers.Conv2D(64, (3, 3), padding='same', activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    layers.Dense(num_classes, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_with_l2.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "model_with_l2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons procéder à l'entraînement de cette version du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "history = model_with_l2.fit(\n",
    "    train_set,\n",
    "    validation_data = test_set,\n",
    "    epochs = epochs\n",
    ")\n",
    "\n",
    "accuracy = history.history['accuracy']\n",
    "validation_accuracy = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "validation_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, accuracy, label='Training Accuracy')\n",
    "plt.plot(epochs_range, validation_accuracy, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, validation_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style=\"color:yellow;\">TODO</b>\n",
    "\n",
    "Analyser les résultats obtenus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "──────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.5. Dropout & Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous combinons ici Dropout et Early Stopping afin de tester leur impact conjoint sur la régularisation du modèle. L'objectif est de réduire l'overfitting tout en optimisant la durée d'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "model_with_dropout_early_stopping = Sequential([\n",
    "    layers.Rescaling(1./255),\n",
    "    layers.Conv2D(16, (3, 3), padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Conv2D(32, (3, 3), padding='same', activation='relu'),\n",
    "    layers.Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_classes, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_with_dropout_early_stopping.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "model_with_dropout_early_stopping.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons procéder à l'entraînement de cette version du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "history = model_with_dropout_early_stopping.fit(\n",
    "    train_set,\n",
    "    validation_data = test_set,\n",
    "    epochs = epochs,\n",
    "    callbacks = [early_stopping]\n",
    ")\n",
    "\n",
    "accuracy = history.history['accuracy']\n",
    "validation_accuracy = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "validation_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, accuracy, label='Training Accuracy')\n",
    "plt.plot(epochs_range, validation_accuracy, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, validation_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style=\"color:yellow;\">TODO</b>\n",
    "\n",
    "Analyser les résultats obtenus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "──────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.6. Elastic Net (L1 + L2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant tester [Elastic Net](https://www.tensorflow.org/api_docs/python/tf/keras/regularizers/L1L2), qui combine les avantages de L1 (Lasso) et L2 (Ridge). Cette régularisation permet à la fois de forcer certains poids à zéro (effet L1) et de réduire la magnitude des autres (effet L2), ce qui équilibre la simplification du modèle et sa stabilité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_elasticnet = Sequential([\n",
    "    layers.Rescaling(1./255),\n",
    "    layers.Conv2D(16, (3, 3), padding='same', activation='relu', kernel_regularizer=keras.regularizers.l1_l2(0.005, 0.005)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(32, (3, 3), padding='same', activation='relu', kernel_regularizer=keras.regularizers.l1_l2(0.005, 0.005)),\n",
    "    layers.Conv2D(64, (3, 3), padding='same', activation='relu', kernel_regularizer=keras.regularizers.l1_l2(0.005, 0.005)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    layers.Dense(num_classes, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_with_elasticnet.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "model_with_elasticnet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons procéder à l'entraînement de cette version du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "history = model_with_elasticnet.fit(\n",
    "    train_set,\n",
    "    validation_data = test_set,\n",
    "    epochs = epochs\n",
    ")\n",
    "\n",
    "accuracy = history.history['accuracy']\n",
    "validation_accuracy = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "validation_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, accuracy, label='Training Accuracy')\n",
    "plt.plot(epochs_range, validation_accuracy, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, validation_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style=\"color:yellow;\">TODO</b>\n",
    "\n",
    "Analyser les résultats obtenus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. <a id='final'>Modèle final</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style=\"color:yellow;\">TODO</b>\n",
    "\n",
    "Indiquer et justifier le choix du modèle final à l'aide des résultats observés. Faire la passe sur ses caractéristiques, à savoir :\n",
    "- Paramètres\n",
    "- Fonction de perte\n",
    "- Algorithme d'optimisation utilisé pour l'entraînement\n",
    "\n",
    "Inclure un schéma du modèle réalisé grâce à cet [outil](https://alexlenail.me/NN-SVG/LeNet.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. <a id='conclusion'>Conclusion</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style=\"color:yellow;\">TODO</b>\n",
    "\n",
    "Écrire la conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
