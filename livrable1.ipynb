{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> **Livrable n°1 : Classification binaire** </center>\n",
    "\n",
    "‎ \n",
    "\n",
    "Réalisé par le **groupe n°2** :\n",
    "- BERTHO Lucien\n",
    "- BOSACKI Paul\n",
    "- GAURE Warren\n",
    "- GRENOUILLET Théo\n",
    "- VALLEMONT Hugo\n",
    "\n",
    "\n",
    "‎\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Sommaire**\n",
    "\n",
    "1. [Mise en contexte](#contexte)\n",
    "2. [Objectif du livrable](#objectif)\n",
    "3. [Démarche suivie](#demarche)\n",
    "4. [Importation des bibliothèques](#import)\n",
    "5. [Adaptation pour GPU](#gpu)\n",
    "6. [Préparation et chargement des données](#load)\n",
    "7. [Exploration des données](#exploration)\n",
    "8. [Configuration de l'environnement](#configuration)\n",
    "9. [Choix de l'architecture](#architecture)\n",
    "10. [Réalisation du modèle](#modele)\n",
    "11. [Entraînement et évaluation du modèle](#train)\n",
    "12. [Amélioration du modèle](#amelioration)\n",
    "13. [Première version complète du modèle CNN](#first-version)\n",
    "14. [Seconde version du modèle CNN](#second-version)\n",
    "15. [Modèle de classification binaire](#binaire)\n",
    "16. [Comparatif des modèles](#comparaison)\n",
    "17. [Calcul des métriques de performance](#metrics)\n",
    "18. [Conclusion](#conclusion)\n",
    "\n",
    "‎ \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. <a id='contexte'>Mise en contexte</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L’entreprise TouNum est spécialisée dans la numérisation de documents, qu’il s’agisse de textes ou d’images. Ses services sont particulièrement sollicités par des entreprises cherchant à transformer leur base documentaire papier en fichiers numériques exploitables. Aujourd’hui, TouNum souhaite aller plus loin en enrichissant son offre avec des outils basés sur le Machine Learning.\n",
    "\n",
    "En effet, certains clients disposent d’un volume considérable de documents à numériser et expriment un besoin croissant pour des solutions de catégorisation automatique. Une telle innovation leur permettrait d’optimiser le traitement et l’exploitation de leurs données numérisées. Toutefois, TouNum ne dispose pas en interne des compétences nécessaires pour concevoir et mettre en place ces technologies.\n",
    "\n",
    "C’est dans ce cadre que notre équipe de spécialistes en Data Science du CESI est sollicitée. Notre mission consiste à développer une première solution intégrant du captioning automatique : un système capable d’analyser des photographies et de générer une légende descriptive de manière autonome.\n",
    "\n",
    "Heureusement, TouNum possède déjà plusieurs milliers d’images annotées, ce qui constituera une ressource précieuse pour entraîner les modèles de Machine Learning à partir d’un apprentissage supervisé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. <a id='objectif'>Objectif du livrable</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TouNum souhaite automatiser la sélection des photos destinées à l'annotation. Ce livrable propose une méthode de classification basée sur les réseaux de neurones pour filtrer les images qui ne sont pas des photos. La solution reposera sur l'architecture de réseau retenue en fonction des résultats obtenus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. <a id='demarche'>Démarche suivie</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour ce livrable, nous avons fait le choix de représenter la démarche que nous avons décidé de suivre sous la forme d'un pipeline. Celui-ci représente les diverses grandes étapes que nous avons réalisé pour parvenir au but mentionné dans la partie précédente.\n",
    "\n",
    "Nous avons choisi de séparer nos réflexions ainsi que nos actions en quatre catégories principales. la **Préparation des données**, l'**Exploration des données**, la **Réalisation du modèle** et l'**Amélioration du modèle**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Pipeline](./results/pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. <a id='import'>Importation des bibliothèques</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "import os\n",
    "import shutil\n",
    "import datetime\n",
    "import keras_tuner as kt\n",
    "import pandas as pd\n",
    "import visualkeras\n",
    "import time\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. <a id='gpu'>Adaptation pour GPU</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin d'entraîner nos modèles sur le GPU de nos ordinateurs, une configuration est nécéssaire. Celle-ci va optimiser la demande de mémoire pour qu'elle soit allouée de manière croissante. Cela va permettre d'éviter d'allouer directement le maximum dès le début et éviter une surutilisation de celle-ci."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            details = tf.config.experimental.get_device_details(gpu)\n",
    "            print(f\"Nom du GPU détecté : {details.get('device_name', 'Nom inconnu')}\")\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. <a id='load'>Préparation et chargement des données</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois les bibliothèques importées, nous pouvons commencer à préparer le terrain en amont et charger les données pour qu'elles puissent être utilisées dans notre pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1. <a>Filtrage des données</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans un premier temps, nous devons veiller à ce que nous aillons bien reçu uniquement des images, c'est-à-dire vérifier qu'il n'y ait pas d'intrus comme des fichiers textes ou autres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_directory = \"dataset_livrable_1/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_image(filename):\n",
    "    try:\n",
    "        with Image.open(filename) as img:\n",
    "            img.verify()\n",
    "        return True\n",
    "    except (UnidentifiedImageError, OSError):\n",
    "        return False\n",
    "\n",
    "def move_non_images(directory):\n",
    "    dump_directory = \"dump\"\n",
    "    os.makedirs(dump_directory, exist_ok = True)\n",
    "    \n",
    "    for folder, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(folder, file)\n",
    "            if not is_image(file_path):\n",
    "                print(f\"Déplacement de {file_path} dans le dossier dump/\")\n",
    "                dest_path = os.path.join(dump_directory, file)\n",
    "                try:\n",
    "                    shutil.move(file_path, dest_path)\n",
    "                except:\n",
    "                    print(\"Erreur lors du déplacement\")\n",
    "                \n",
    "move_non_images(dataset_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "──────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2. <a>Vérification des images</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tout d'abord, nous devons nous assurer du bon état des images reçues, c'est-à-dire vérifier si elles n'ont pas été corrompues ou mal formatées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_image(path):\n",
    "    try:\n",
    "        img_raw = tf.io.read_file(path)\n",
    "        _ = tf.image.decode_image(img_raw, channels=3)\n",
    "        return (path, True)\n",
    "    except Exception:\n",
    "        return (path, False)\n",
    "\n",
    "def clean_corrupted_images(directory, extensions=(\"jpg\", \"jpeg\", \"png\"), max_workers=8):\n",
    "    image_paths = []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(extensions):\n",
    "                image_paths.append(os.path.join(root, file))\n",
    "\n",
    "    print(f\"Scan de {len(image_paths)} images dans {directory}\")\n",
    "\n",
    "    corrupted_count = 0\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [executor.submit(is_valid_image, path) for path in image_paths]\n",
    "        for future in as_completed(futures):\n",
    "            path, is_valid = future.result()\n",
    "            if not is_valid:\n",
    "                try:\n",
    "                    os.remove(path)\n",
    "                    corrupted_count += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Erreur de suppression {path} : {e}\")\n",
    "\n",
    "    print(f\"Vérification terminée : {corrupted_count} image(s) corrompue(s) supprimée(s).\")\n",
    "    \n",
    "\n",
    "clean_corrupted_images(dataset_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "──────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3. <a>Gestion des logs</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous créons ici un dossier qui va nous permettre de stocker les logs qui seront utilisés par [TensorBoard](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "──────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4. <a>Chargement des images</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les images doivent être séparées en deux ensembles : un pour l'entraînement du modèle, l'autre pour son évaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_h = 128\n",
    "image_w = 128\n",
    "batch_s = 16\n",
    "\n",
    "train_set, test_set = keras.utils.image_dataset_from_directory(\n",
    "    dataset_directory,\n",
    "    label_mode = \"int\",\n",
    "    batch_size = batch_s,\n",
    "    image_size = (image_h, image_w),\n",
    "    seed = SEED,\n",
    "    validation_split = 0.2,\n",
    "    subset = \"both\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. <a id='exploration'>Exploration des données</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que les données ont pu être préparées et chargées, nous pouvons nous intéresser de plus près à elles, à commencer par le nom des classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1. <a>Nom des classes</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train_set.class_names\n",
    "print(f\"Classes détectées : {class_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous retrouvons bien les 5 classes attendues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "──────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2. <a>Répartition des données</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous regardons maintenant la répartition des données entre les classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_class_distribution(dataset, name):\n",
    "    label_counts = collections.Counter(label.numpy() for _, label in dataset.unbatch())\n",
    "\n",
    "    classes = {0: \"peintures\", 1: \"photos\", 2: \"schémas\", 3: \"croquis\", 4: \"textes scannés\"}\n",
    "\n",
    "    total = sum(label_counts.values())\n",
    "  \n",
    "    labels = []\n",
    "    counts = []\n",
    "    percentages = []\n",
    "\n",
    "    for label_id in sorted(label_counts):\n",
    "        class_name = classes.get(label_id, f\"Classe inconnue ({label_id})\")\n",
    "        count = label_counts[label_id]\n",
    "        labels.append(class_name)\n",
    "        counts.append(count)\n",
    "        percentages.append(count / total * 100)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(labels, counts)\n",
    "\n",
    "    for bar, pct in zip(bars, percentages):\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width() / 2, height + 1, f\"{pct:.1f}%\", ha='center', va='bottom')\n",
    "\n",
    "    plt.title(f\"Répartition des classes ({name}_set - {total} images)\")\n",
    "    plt.xlabel(\"Classe\")\n",
    "    plt.ylabel(\"Nombre d'images\")\n",
    "    plt.xticks(rotation=15)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_class_distribution(train_set, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_class_distribution(test_set, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme on peut l'observer sur les histogrammes générés, la répartition des données entre les différentes classes est déséquilibrée dans les deux ensembles. Bien que cela ne soit pas gênant dans le set de test car nous souhaitons avoir des conditions proches de la réalité, cela peut poser problème lors de l'entraînement de notre modèle, qui risque d'être biaisé envers les classes majoritaires."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "──────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3. <a>Taille des données</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous affichons maintenant la taille des données, information pouvant être utile par la suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(train_set.take(1)))\n",
    "print(f\"Tensor des images : {images.shape}\")\n",
    "print(f\"Tensor des labels : {labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "──────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.4. <a>Affichage des images</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous affichons quelques images pour voir plus en détail ce à quoi nous avons affaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8, 8))\n",
    "for images, labels in train_set.take(10):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(class_names[labels[i].numpy()])\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "──────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.5. <a>Résolution originelle des images</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous affichons la résolution originelle de quelques images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = []\n",
    "for root, dirs, files in os.walk(dataset_directory):\n",
    "    for file in files:\n",
    "        if len(image_paths) <= 1000:\n",
    "            image_paths.append(os.path.join(root, file))\n",
    "\n",
    "widths, heights = [], []\n",
    "for path in image_paths:\n",
    "    try:\n",
    "        with Image.open(path) as img:\n",
    "            width, height = img.size\n",
    "            widths.append(width)\n",
    "            heights.append(height)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(widths, bins = 30, color='skyblue', edgecolor='black')\n",
    "plt.title(\"Distribution des largeurs des images\")\n",
    "plt.xlabel(\"Largeur (pixels)\")\n",
    "plt.ylabel(\"Nombre d'images\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(heights, bins = 30, color='skyblue', edgecolor='black')\n",
    "plt.title(\"Distribution des hauteurs des images\")\n",
    "plt.xlabel(\"Hauteur (pixels)\")\n",
    "plt.ylabel(\"Nombre d'images\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. <a id='configuration'>Configuration de l'environnement</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour optimiser les performances des calculs, nous allons configurer les données à l’aide de deux fonctions : `Dataset.cache` et `Dataset.prefetch`.  \n",
    "- [`Dataset.cache`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#cache) stocke les données en mémoire pour éviter les accès répétés au disque.  \n",
    "- [`Dataset.prefetch`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#prefetch) permet de traiter un élément en arrière-plan pendant l'entraînement ou l'évaluation.  \n",
    "\n",
    "En combinant ces techniques, nous réduirons significativement le temps de traitement et la charge computationnelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not gpus:\n",
    "    AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "    train_set = train_set.cache().shuffle(1000).prefetch(buffer_size = AUTOTUNE)\n",
    "    test_set = test_set.cache().prefetch(buffer_size = AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. <a id='architecture'>Choix de l'architecture</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les **Convolutional Neural Networks (CNN)** sont devenus l’architecture de référence pour les tâches de classification d’images, notamment en classification multi-classes. Leur efficacité repose sur leur capacité à exploiter la structure spatiale locale des images à travers des opérations de convolution, permettant ainsi une extraction hiérarchique des caractéristiques visuelles (bords, formes, textures…).\n",
    "\n",
    "Historiquement, LeCun et al. (1998) ont démontré la pertinence des CNN dans la reconnaissance de chiffres manuscrits avec LeNet-5. Cette approche a été fortement étendue avec AlexNet (Krizhevsky et al., 2012), qui a surpassé toutes les autres méthodes sur le défi ImageNet, impliquant la classification dans 1000 classes différentes. Depuis, des architectures plus profondes comme VGG, ResNet ou EfficientNet ont confirmé la domination des CNN dans ce domaine (Rawat & Wang, 2017).\n",
    "\n",
    "De nombreux frameworks modernes (TensorFlow, PyTorch) proposent des implémentations standardisées de CNN pour la classification multi-classes, et les performances obtenues dépassent largement celles des méthodes classiques (SVM, k-NN, etc.) sur des datasets variés.\n",
    "\n",
    "En résumé, le choix d’un CNN est justifié par :\n",
    "- Sa capacité à apprendre automatiquement des représentations visuelles pertinentes\n",
    "- Son efficacité démontrée sur des benchmarks multi-classes (ex : CIFAR-10, ImageNet)\n",
    "- Sa large adoption dans la recherche et l’industrie pour les tâches de vision par ordinateur\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "**<u>Sources</u>**\n",
    "1. Lecun, Yann & Bottou, Leon & Bengio, Y. & Haffner, Patrick. (1998). *Gradient-Based Learning Applied to Document Recognition. Proceedings of the IEEE. 86. 2278 - 2324. 10.1109/5.726791.*\n",
    "2. Krizhevsky, Alex & Sutskever, Ilya & Hinton, Geoffrey. (2012). *ImageNet Classification with Deep Convolutional Neural Networks. Neural Information Processing Systems. 25. 10.1145/3065386.*\n",
    "3. Rawat, Waseem & Wang, Zenghui. (2017). *Deep Convolutional Neural Networks for Image Classification: A Comprehensive Review. Neural Computation. 29. 2352-2449. 10.1162/NECO_a_00990.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. <a id='modele'>Réalisation du modèle</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que le choix de l'architecture est fait, nous pouvons commencer à créer le modèle que nous allons utiliser pour classifier les images envoyées par l'entreprise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.1 <a id='modele'>Création du modèle de base</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre modèle sera structuré autour des blocs suivants :  \n",
    "- Une **couche de rescaling** pour normaliser les valeurs des composantes RGB des pixels dans l'intervalle [0;1].  \n",
    "- Une **première convolution** avec 16 filtres de taille 3x3 (`Conv2D`), suivie d'un **max pooling** pour réduire la dimension spatiale.  \n",
    "- Une **seconde convolution** utilisant 32 filtres de taille 3x3.  \n",
    "- Une **troisième convolution** avec 64 filtres de taille 3x3.  \n",
    "- Une **transformation en vecteur** via une opération d'aplatissement (`Flatten`).  \n",
    "- Une **couche dense** de 128 unités pour capturer les caractéristiques extraites.  \n",
    "- Enfin, une **sortie entièrement connectée** avec 1 unité, correspondant à la classe cible.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(class_names)\n",
    "\n",
    "def create_model(use_dropout = False, show_summary = True, hparams = None):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(layers.Rescaling(1.0 / 255))\n",
    "    \n",
    "    num_units = hparams.get('units', 128) if hparams and hparams.get('units', 128) != None else 128\n",
    "    activation = hparams.get('activation', 'relu') if hparams and hparams.get('activation', 'relu') != None else 'relu'\n",
    "    dropout_rate = hparams.get('dropout', 0.5) if hparams and hparams.get('dropout', 0.5) != None else 0.5\n",
    "\n",
    "    model.add(layers.Conv2D(16, (3, 3), padding = 'same', activation = activation))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    if use_dropout:\n",
    "        model.add(layers.Dropout(dropout_rate))\n",
    "\n",
    "    model.add(layers.Conv2D(32, (3, 3), padding = 'same', activation = activation))\n",
    "    model.add(layers.Conv2D(64, (3, 3), padding = 'same', activation = activation))\n",
    "\n",
    "    if use_dropout:\n",
    "        model.add(layers.Dropout(dropout_rate))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(num_units, activation = activation))\n",
    "\n",
    "    if use_dropout:\n",
    "        model.add(layers.Dropout(dropout_rate))\n",
    "\n",
    "    model.add(layers.Dense(num_classes, activation = 'softmax'))\n",
    "\n",
    "    learning_rate = hparams.get('lr') if hparams and hparams.get('lr') != None else 0.001\n",
    "    optimizer = keras.optimizers.Adam(learning_rate = learning_rate) if hparams else 'adam'\n",
    "\n",
    "    model.compile(\n",
    "        optimizer = optimizer,\n",
    "        loss = keras.losses.SparseCategoricalCrossentropy(from_logits = False),\n",
    "        metrics = ['accuracy']\n",
    "    )\n",
    "\n",
    "    if show_summary:\n",
    "        model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'optimiseur [`Adam`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam) est choisi pour sa capacité d'adaptation et sa rapidité de convergence. La fonction de perte [`SparseCategoricalCrossentropy`](https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy), quant à elle, est utilisée car jugée plus efficace en mémoire que d'autres fonctions et bien adaptée à la classification multi-classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A l'aide de la bibliothèque \"visualkeras\", nous avons pu réaliser un schéma représentant le modèle que nous avons créé.\n",
    "\n",
    "![modele](./model_imgs/basic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "──────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.2 <a id='modele'>Tuning des hyperparamètres</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette section, nous allons effectuer le tuning des hyperparamètres, c’est-à-dire le réglage manuel ou automatique des paramètres qui contrôlent le comportement du modèle, comme le taux d’apprentissage, la taille des couches ou la taille des batchs. Cela permet d’optimiser les performances du modèle en trouvant la combinaison de paramètres qui offre les meilleurs résultats sur les données de validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plus précisément, nous cherchons à optimiser les paramètres suivants :\n",
    "- L'usage (ou non) des couches de **Dropout**\n",
    "- Le **pas d'apprentissage** (learning rate)\n",
    "- La **fonction d'activation** donnant de meilleurs résultats entre [`relu`](https://www.tensorflow.org/api_docs/python/tf/keras/activations/relu) et [`tanh`](https://www.tensorflow.org/api_docs/python/tf/keras/activations/tanh)\n",
    "- Le **nombre d'epochs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour ce faire, nous utilisons l'algorithme [`Hyperband`](https://keras.io/keras_tuner/api/tuners/hyperband/) de la librairie [Keras Tuner](https://keras.io/keras_tuner/) pour chercher ces hyperparamètres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    units = hp.Int(\"units\", min_value = 32, max_value = 256, step = 32)\n",
    "    activation = hp.Choice(\"activation\", [\"relu\", \"tanh\"])\n",
    "    dropout = hp.Boolean(\"dropout\")\n",
    "\n",
    "    hparams = {\n",
    "        \"dense_units\": units,\n",
    "        \"activation\": activation,\n",
    "        \"dropout_3\": 0.5 if dropout else 0.0 \n",
    "    }\n",
    "\n",
    "    model = create_model(\n",
    "        use_dropout = dropout,  \n",
    "        show_summary = False,\n",
    "        hparams = hparams\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "tuner = kt.Hyperband(\n",
    "    hypermodel = build_model,\n",
    "    objective = 'val_accuracy',\n",
    "    max_epochs = 10,\n",
    "    factor = 3,\n",
    "    directory = 'hyperband',\n",
    "    project_name = 'hyperband_test'\n",
    ")\n",
    "\n",
    "stop_early = EarlyStopping(\n",
    "    monitor = 'val_accuracy',\n",
    "    patience = 5,\n",
    "    restore_best_weights = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A l'aide de la bibliothèque \"visualkeras\", nous avons pu réaliser un schéma représentant le modèle que nous avons créé.\n",
    "\n",
    "![modele](./model_imgs/hyperband.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. <a id='train'>Entraînement et évaluation du modèle</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec le modèle créé, nous pouvons désormais procéder à son entraînement et à son évaluation avec les ensembles de données à notre disposition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.1. <a>Graphiques</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous utilisons des graphiques afin de visualiser les courbes d’accuracy pour suivre en temps réel les performances du modèle sur les données d’entraînement et de validation. Cela permet de détecter rapidement les signes de surapprentissage ou de sous-apprentissage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant de générer les graphiques, nous créons les deux callbacks suivants :\n",
    "- [`TensorBoard`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard?hl=en) : Il permet de visualiser en temps réel l’évolution des métriques et du modèle, facilitant l’analyse et le suivi de l’entraînement.\n",
    "- [`ModelCheckpoint`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint?hl=en) : Ce callback permet de sauvegarder automatiquement le meilleur modèle au cours de l’entraînement, évitant ainsi de perdre les meilleures performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = TensorBoard(\n",
    "    log_dir = log_dir,\n",
    "    histogram_freq = 1\n",
    ")\n",
    "\n",
    "checkpoint_callback_acc = ModelCheckpoint(\n",
    "    filepath = 'checkpoints/best_model_acc.keras',\n",
    "    monitor = 'val_accuracy',\n",
    "    save_best_only = True,\n",
    "    save_weights_only = False,\n",
    "    mode = 'max',\n",
    "    verbose = 1\n",
    ")\n",
    "checkpoint_callback_loss = ModelCheckpoint(\n",
    "    filepath = 'checkpoints/best_model_loss.keras',\n",
    "    monitor = 'val_loss',\n",
    "    save_best_only = True,\n",
    "    save_weights_only = False,\n",
    "    mode = 'min',\n",
    "    verbose = 1\n",
    ")\n",
    "\n",
    "callbacks.append(tensorboard_callback)\n",
    "callbacks.append(checkpoint_callback_acc)\n",
    "callbacks.append(checkpoint_callback_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme nous l'avons observé lors de la phase d'exploration des données, il y a un déséquilibre notable dans la répartition des données entre les classes des deux ensembles. Pour palier à ce problème lors de l'entraînement de notre modèle, nous allons ajouter des poids aux classes, qui seront générés grâce à la méthode [`compute_class_weight`](https://scikit-learn.org/stable/modules/generated/sklearn.utils.class_weight.compute_class_weight.html) de la librairie [scikit-learn](https://scikit-learn.org/stable/index.html). Ceci aura pour effet de renforcer l’importance des classes minoritaires lors de l’apprentissage, afin que le modèle ne privilégie pas uniquement les classes majoritaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array([label.numpy() for _, label in train_set.unbatch()])\n",
    "class_weights = compute_class_weight(class_weight = \"balanced\", classes = np.unique(y_train), y = y_train)\n",
    "weights_dict = {cls: weight for cls, weight in zip(np.unique(y_train), class_weights)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_set = train_set, test_set = test_set, epochs = 10, weights = weights_dict, use_hyperparameters = False, Name = \"Name\"):\n",
    "    history = None\n",
    "    start = time.time()\n",
    "    print(f\"Début de l'entraînement : {start}\")\n",
    "    if use_hyperparameters:\n",
    "        train_size = int(0.8 * len(train_set))\n",
    "        val_size = len(train_set) - train_size\n",
    "        train_dataset = train_set.take(train_size)\n",
    "        val_dataset = train_set.skip(val_size)\n",
    "        \n",
    "        history = model.fit(\n",
    "            train_dataset,\n",
    "            validation_data = val_dataset,\n",
    "            epochs = epochs,\n",
    "            validation_split = 0.2\n",
    "        )\n",
    "    \n",
    "    else:\n",
    "        history = model.fit(\n",
    "            train_set,\n",
    "            validation_data = test_set,\n",
    "            epochs = epochs,\n",
    "            callbacks = callbacks,\n",
    "            class_weight = weights\n",
    "        )\n",
    "    end = time.time()\n",
    "    print(f\"Fin de l'entraînement : {end}\")\n",
    "    print(f\"Durée de l'entraînement : {end - start}\")\n",
    "    with open(f\"training_time.txt\", \"a\") as f:\n",
    "        f.write(f\"{Name},{end - start}\\n\")\n",
    "\n",
    "    hist_df = pd.DataFrame(history.history) \n",
    "    hist_csv_file = f'history_{Name}.csv'\n",
    "    with open(f\"results_csv/{hist_csv_file}\", mode='w') as f:\n",
    "        hist_df.to_csv(f)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Résultats de base](./results/base_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyse des résultats :**\n",
    "- La courbe d'accuracy d'entraînement montant rapidement jusqu'à presque 100% montre que le modèle apprend très bien sur les données d'entraînement.\n",
    "- L'accuracy sur le jeu de validation stagne autour des 85-86%.\n",
    "- La courbe de loss d'entraînement diminue progressivement, ce qui est attendu quand le modèle apprend bien.\n",
    "- La courbe de loss de validation augmente drastiquement à partir de la seconde époque.\n",
    "\n",
    "Il semblerait que le modèle présente des signes de surapprentissage, de par le fait qu'il mémorie trop bien les données d'entraînement, sans pour autant bien généraliser sur les données de validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "──────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.2. <a>Matrice de confusion</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous utilisons une matrice de confusion pour évaluer plus finement les performances d’un modèle en montrant les erreurs de classification pour chaque classe. Elle met en évidence les classes confondues et aide à cibler les axes d’amélioration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "y_true = []\n",
    "\n",
    "for images, labels in test_set:\n",
    "    X_test.append(images)\n",
    "    y_true.append(labels)\n",
    "\n",
    "X_test = np.concatenate(X_test)\n",
    "y_true = np.concatenate(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_matrix(model, X_test = X_test, y_true = y_true, class_names = class_names):\n",
    "    y_pred_proba = model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred_proba, axis = 1)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    display = ConfusionMatrixDisplay(cm, display_labels = class_names)\n",
    "    display.plot(cmap = plt.cm.Blues)\n",
    "    plt.title(\"Matrice de confusion\")\n",
    "    plt.xticks(rotation = 45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_matrix(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Matrice des résultats de base](./results/base_matrix.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyse des résultats :**\n",
    "- Le modèle parvient à classer correctement une grande majorité des images pour chaque classe, notamment pour les classes Text (1929 prédictions correctes) et Schematics (1782).\n",
    "- On observe toutefois une confusion notable entre les classes Painting et Photo, avec respectivement 359 Paintings prédites comme Photo et 430 Photos prédites comme Painting.\n",
    "- Cette confusion peut s'expliquer par une proximité visuelle ou stylistique entre certaines photos et peintures réalistes, que le modèle peine à distinguer.\n",
    "- Les classes Sketch et Text sont beaucoup mieux différenciées, ce qui peut être dû à leurs caractéristiques visuelles très marquées (traits fins ou présence de texte)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "──────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.3. <a>Images en erreurs</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous affichons quelques images qui n'ont pas été bien prédite (vrai négatif ou faux positif) pour essayer de visualiser et comprendre pourquoi elles sont en erreurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_errors(model, test_set = test_set, class_names = class_names):  \n",
    "    y_true = []  \n",
    "    y_pred = []  \n",
    "    all_images = []  \n",
    "\n",
    "    for batch in test_set:\n",
    "        images, labels = batch  \n",
    "        predictions = model.predict(images, verbose=0)  \n",
    "        y_true.extend(labels.numpy()) \n",
    "        y_pred.extend(predictions.argmax(axis=1)) \n",
    "        all_images.extend(images.numpy()) \n",
    "\n",
    "    # Convertissez y_true, y_pred et all_images en tableaux NumPy pour une manipulation plus facile\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    all_images = np.array(all_images)\n",
    "\n",
    "    # Identifiez les indices des erreurs\n",
    "    errors = [(i, true, pred) for i, (true, pred) in enumerate(zip(y_true, y_pred)) if true != pred]\n",
    "\n",
    "    # Affichez les images mal prédites pour chaque classe\n",
    "    for class_id in set(y_true):\n",
    "        print(f\"{class_names[class_id]}\")\n",
    "        class_errors = [e for e in errors if e[1] == class_id]\n",
    "\n",
    "        if not class_errors:\n",
    "            print(f\"Aucune erreur pour {class_names[class_id]}\")\n",
    "            continue\n",
    "        \n",
    "        # Limitez à 5 exemples maximum\n",
    "        class_errors = class_errors[:5]\n",
    "\n",
    "        # Affichez quelques exemples\n",
    "        fig, axes = plt.subplots(1, len(class_errors), figsize=(15, 5))\n",
    "        for ax, (idx, true, pred) in zip(axes, class_errors):\n",
    "            if idx >= len(all_images):  # Vérifiez si l'indice est valide\n",
    "                print(f\"Indice {idx} hors limites pour les images.\")\n",
    "                continue\n",
    "            ax.imshow(all_images.astype(\"uint8\")[idx])  # Affichez l'image depuis le tableau NumPy\n",
    "            ax.set_title(f\"prédit : {class_names[pred]}\")\n",
    "            ax.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme on peut le voir, les peintures et les photos sont souvent confondu par le modèle. On retrouve aussi des textes et des sketchs qui ont été mis dans schématiques. Les schématiques,  quand ils sont colorés, ont été confondus avec des peintures ou des photos. Quand ils sont noir et blanc, ils sont classés en sketchs ou en textes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "──────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.4. <a>TensorBoard</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous utilisons TensorBoard pour visualiser de manière interactive l’entraînement du modèle, en suivant l’évolution des métriques, la structure du réseau et d’autres informations utiles pour le debug et l’optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. <a id='amelioration'>Amélioration du modèle</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin de palier au surapprentissage observé et d’améliorer la généralisation du modèle, plusieurs techniques de régularisation ont été retenues :\n",
    "\n",
    "- [Data Augmentation](#augmentation): Cette technique consister à générer artificiellement de nouvelles images en appliquant des transformations aléatoires aux données existantes. Elle permet d'améliorer la généralisation du modèle en le rendant plus robuste aux variations comme l’orientation, la luminosité ou le zoom.\n",
    "\n",
    "- [Dropout](#dropout) : Cette méthode consiste à désactiver aléatoirement un certain pourcentage de neurones à chaque itération lors de l'entraînement. Cela empêche le modèle de devenir trop dépendant de certaines connexions et encourage l'apprentissage de représentations plus robustes. Une valeur typique se situe entre 0.2 et 0.5 selon la complexité du réseau.\n",
    "\n",
    "- [Early-Stopping](#early-stopping) : Cette technique permet d'arrêter automatiquement l'entraînement lorsque la performance sur l’ensemble de validation commence à se dégrader. Elle évite d’entraîner le modèle trop longtemps, ce qui pourrait mener à un surajustement aux données d’entraînement. Un paramètre clé est la `patience`, qui définit le nombre d’époques d'attente avant d'interrompre l'entraînement si aucune amélioration n'est observée.\n",
    "\n",
    "En testant et, potentiellement, combinant ces différentes approches, nous parviendrons à obtenir un modèle plus stable, robuste, et capable de mieux généraliser sur des données non vues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12.1. <a id='augmentation'>Data Augmentation</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour cette approche, nous allons appliquer des transformations aux données d'entraînement, comme un retournement aléatoire, une rotation de 10% et d’un zoom vertical de 10%. Les données de test restent inchangées pour permettre au modèle de pouvoir faire des prédictions sur un cas réel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential([\n",
    "    layers.RandomFlip(input_shape = (image_h, image_w, 3), mode = 'horizontal_and_vertical'),\n",
    "    layers.RandomRotation(factor = 0.1, fill_mode = 'nearest'),\n",
    "    layers.RandomZoom(height_factor = 0.1, fill_mode = 'nearest'),\n",
    "])\n",
    "\n",
    "augmented_train_set = train_set.map(lambda x, y: (data_augmentation(x, training = True), y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A l'aide de la bibliothèque \"visualkeras\", nous avons pu réaliser un schéma représentant le modèle que nous avons créé.\n",
    "\n",
    "![modele](./model_imgs/data_augmentation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En procédant à l'entraînement de ce modèle (retrouvable dans [cette partie](#comparaison) de ce livrable), nous obtenons les graphiques suivants :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Résultats avec Data Augmentation](./results/augmentation_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyse des résultats :**\n",
    "- La courbe d’accuracy d’entraînement progresse de manière régulière jusqu’à environ 88%, ce qui montre que le modèle apprend progressivement les données, bien qu’un peu plus lentement que le modèle de base.\n",
    "- L’accuracy sur le jeu de validation atteint jusqu’à 86%, avec des variations légères mais une meilleure stabilité que dans le modèle de base.\n",
    "- La loss d’entraînement diminue de façon constante, comme attendu.\n",
    "- Contrairement au modèle de base, la courbe de loss de validation ne présente pas d’augmentation brutale, ce qui laisse supposer que le modèle généralise mieux aux nouvelles données.\n",
    "\n",
    "L'ajout de Data Augmentation semble avoir permis de réduire le surapprentissage, en exposant le modèle à davantage de variations. Même si l’accuracy d’entraînement est légèrement inférieure à celle du modèle de base, l’accuracy de validation est équivalente voire meilleure, ce qui est un bon signe de robustesse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "──────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Matrice de confusion avec Data Augmentation](./results/augmentation_matrix.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyse des résultats :**\n",
    "- Le modèle reconnaît globalement très bien toutes les classes, en particulier Text (1924 prédictions correctes) et Schematics (1803), avec des performances comparables ou légèrement supérieures à celles du modèle de base.\n",
    "- La classe Photo est mieux reconnue que précédemment : seulement 332 erreurs en Painting contre 430 dans le modèle de base, ce qui témoigne d’une meilleure distinction entre photos et peintures.\n",
    "- On observe encore une confusion persistante entre Painting et Photo, bien que réduite. Par exemple, 444 peintures sont prédites comme photos, ce qui reste la confusion dominante.\n",
    "- Quelques erreurs sont visibles entre Schematics et Text (61 confusions dans ce sens), ce qui peut être lié à des éléments visuels similaires, comme des lignes ou des formes proches.\n",
    "\n",
    "La Data Augmentation a permis au modèle de mieux généraliser, en réduisant certaines confusions clés comme entre Photo et Painting. La matrice de confusion confirme que l’amélioration constatée sur les courbes d’accuracy se traduit par une meilleure robustesse sur plusieurs classes, tout en conservant d’excellents résultats sur les classes les plus distinctives (Sketch, Text)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "──────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12.2. <a id='dropout'>Dropout</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour cette approche, nous ajoutons les couches de Dropout suivantes :\n",
    "- La première avec un taux de 25% après le MaxPooling\n",
    "- La seconde de 25% aussi après la troisième couche de convolution\n",
    "- La dernière avec un taux de 50% après la première couche Dense.\n",
    "Nous nous sommes inspirés de l’approche de [Keras pour le dataset MNIST](https://github.com/keras-team/keras/blob/keras-2/examples/mnist_cnn.py).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model_with_dropout = \u001b[43mcreate_model\u001b[49m(use_dropout = \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'create_model' is not defined"
     ]
    }
   ],
   "source": [
    "model_with_dropout = create_model(use_dropout = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A l'aide de la bibliothèque \"visualkeras\", nous avons pu réaliser un schéma représentant le modèle que nous avons créé.\n",
    "\n",
    "![modele](./model_imgs/dropout.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En procédant à l'entraînement de ce modèle (retrouvable dans [cette partie](#comparaison) de ce livrable), nous obtenons les graphiques suivants :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Résultats avec Dropout](./results/dropout_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyse des résultats :**\n",
    "- La courbe d’accuracy d’entraînement progresse de manière plus progressive, atteignant environ 93% à la fin de l'entraînement, ce qui reflète l’effet du Dropout qui ralentit volontairement la mémorisation du modèle.\n",
    "- L’accuracy de validation atteint un plateau autour de 88%, avec une progression plus régulière que dans les modèles précédents.\n",
    "- La courbe de loss d’entraînement diminue fortement et régulièrement, ce qui est attendu lorsque le modèle apprend efficacement tout en étant régularisé.\n",
    "- La loss de validation présente quelques oscillations modérées, mais reste globalement stable, sans envolée brutale comme dans le modèle de base.\n",
    "\n",
    "L'ajout de Dropout a eu pour effet de ralentir l'apprentissage, mais aussi de mieux contrôler le surapprentissage. Le modèle présente un bon équilibre entre performance sur le jeu d’entraînement et généralisation, avec une validation stable et une accuracy élevée, ce qui montre que cette régularisation est efficace sur ce problème."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "──────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Matrice de confusion avec Dropout](./results/dropout_matrix.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyse des résultats :**\n",
    "- Le modèle avec Dropout montre une très bonne capacité de classification, notamment sur les classes Text (1933) et Schematics (1868), qui sont reconnues avec une très haute précision.\n",
    "- Les erreurs entre Photo et Painting sont toujours présentes, mais légèrement réduites par rapport au modèle de base (ex. : 388 peintures prédites comme photos contre 444 auparavant).\n",
    "- Les classes Sketch et Text restent très bien distinguées des autres, avec très peu de confusion, ce qui confirme la capacité du modèle à reconnaître des caractéristiques visuelles simples ou marquées.\n",
    "- Globalement, toutes les confusions majeures ont diminué, notamment celles entre Painting et Schematics, ou entre Photo et Schematics.\n",
    "\n",
    "L’intégration du Dropout a permis de renforcer la généralisation du modèle, en réduisant les confusions entre les classes proches sans nuire à la précision globale. Cette matrice montre une amélioration claire par rapport au modèle de base, avec une meilleure stabilité des prédictions sur l’ensemble des classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "──────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12.3. <a id='early-stopping'>Early Stopping</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour cette approche, nous ajoutons un Early Stopping basé sur la perte de validation, avec une patience de 3 épochs, afin d’interrompre l’entraînement dès que le modèle cesse de s’améliorer et de conserver les meilleurs poids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor = 'val_accuracy',\n",
    "    patience = 3,\n",
    "    mode = 'max',\n",
    "    restore_best_weights = True\n",
    ")\n",
    "\n",
    "callbacks.append(early_stopping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En procédant à l'entraînement de ce modèle (retrouvable dans [cette partie](#comparaison) de ce livrable), nous obtenons les graphiques suivants :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Résultats avec Early Stopping](./results/early_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyse des résultats :**\n",
    "- La courbe d’accuracy d’entraînement progresse très rapidement et atteint presque 97% en seulement trois époques, montrant que le modèle a appris très vite.\n",
    "- L’accuracy de validation stagne autour de 86%, avec une légère baisse entre la deuxième et la troisième époque, indiquant que le modèle commence à surapprendre.\n",
    "- La courbe de loss d’entraînement diminue fortement, ce qui est attendu.\n",
    "- En revanche, la loss de validation diminue au début puis augmente à nouveau dès la deuxième époque, ce qui a probablement déclenché l’arrêt anticipé de l’entraînement via le callback `EarlyStopping`.\n",
    "\n",
    "L'utilisation de l’Early Stopping a permis de prévenir un surapprentissage trop important, mais les résultats suggèrent que le modèle n’a pas encore eu le temps d’atteindre son plein potentiel. Il aurait pu bénéficier d’un ou deux cycles supplémentaires avant stagnation réelle. Malgré tout, cette stratégie permet de gagner du temps d’entraînement et de conserver un modèle qui reste stable sur les données de validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "──────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Matrice de confusion avec Early Stopping](./results/early_matrix.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyse des résultats :**\n",
    "- Le modèle montre une bonne capacité de classification, notamment sur les classes Schematics (1878) et Text (1929), qui sont reconnues avec une très grande précision.\n",
    "- La classe Photo est très bien prédite avec 1671 images correctement classées, et une confusion modérée avec Painting (339 erreurs), qui reste une confusion fréquente dans l’ensemble des modèles testés.\n",
    "- La classe Painting présente encore 450 confusions avec Photo, ce qui confirme une fois de plus la difficulté du modèle à distinguer ces deux types d’images.\n",
    "- Les classes Sketch et Text restent très bien différenciées, avec peu de confusions, ce qui témoigne d’une bonne généralisation sur ces catégories aux caractéristiques visuelles marquées.\n",
    "\n",
    "Le modèle avec Early Stopping présente des résultats très satisfaisants, comparables à ceux du modèle avec Dropout seul. Il parvient à bien classer la majorité des images tout en évitant un surapprentissage trop important. L’arrêt anticipé de l’entraînement a permis d’atteindre un bon compromis entre précision et généralisation, bien que les confusions persistantes entre Painting et Photo restent un axe d’amélioration possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "──────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "À la vue des résultats obtenus sur les modèles intermédiaires, il apparaît que l’utilisation conjointe de Data Augmentation, de Dropout et de l’Early Stopping constitue une stratégie pertinente pour améliorer la régularisation du modèle. Ces techniques, prises individuellement, ont permis de limiter le surapprentissage tout en stabilisant les performances sur les données de validation. Leur combinaison vise ainsi à tirer parti de leurs effets complémentaires afin de construire un modèle plus robuste, mieux préparé à généraliser sur des données variées. La section suivante présente les performances du modèle final entraîné avec cette approche."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. <a id='first-version'>Première version complète du modèle CNN</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette section, nous présentons la première version complète du modèle de réseau de neurones convolutif (CNN). Ce modèle intègre les techniques de régularisation vues précédemment, à savoir le dropout, la data augmentation et l'early stopping.\n",
    "\n",
    "L’architecture du modèle atteint un total de 33 583 781 paramètres entraînables, la majeure partie provenant de la couche dense suivant l’aplatissement (Flatten), qui connecte un vecteur de plus de 260 000 dimensions à une couche de 128 neurones.\n",
    "\n",
    "Pour l'entraînement de ce modèle, nous avons utilisé la fonction de perte SparseCategoricalCrossentropy, adaptée aux problèmes de classification multi-classes. L’optimisation est assurée par l’algorithme Adam, reconnu pour sa robustesse et sa capacité à s’adapter dynamiquement au taux d’apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_hyperparameters = True\n",
    "if use_hyperparameters:\n",
    "    train_size = int(0.8 * len(train_set))  # 80% for training\n",
    "    val_size = len(train_set) - train_size  # 20% for validation\n",
    "\n",
    "    train_dataset = train_set.take(train_size)\n",
    "    val_dataset = train_set.skip(train_size)\n",
    "\n",
    "    tuner.search(\n",
    "        train_dataset,\n",
    "        validation_data = val_dataset,\n",
    "        epochs = 50,\n",
    "        validation_split = 0.2,\n",
    "        callbacks = [stop_early]\n",
    "    )\n",
    "    \n",
    "    best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\n",
    "    model = create_model(use_dropout = True, hparams = best_hps.values)\n",
    "else:\n",
    "    model = create_model(use_dropout = True)\n",
    "\n",
    "callbacks.append(early_stopping)\n",
    "model = train_model(model, train_set = augmented_train_set, test_set = test_set, epochs = 20, use_hyperparameters = use_hyperparameters, tuner = tuner,best_hps=best_hps, Name = \"first_model\")\n",
    "\n",
    "display_matrix(model)\n",
    "model.save(\"models/first_model.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Résultats avec le modèle final](./results/final_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A l'aide de la bibliothèque \"visualkeras\", nous avons pu réaliser un schéma représentant le modèle que nous avons créé.\n",
    "\n",
    "![modele](./model_imgs/first_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyse des résultats :**\n",
    "\n",
    "- L’accuracy d’entraînement progresse régulièrement jusqu’à dépasser les 92%, indiquant que le modèle parvient bien à apprendre les données, malgré la régularisation.\n",
    "- L’accuracy de validation reste très proche, autour de 90-91%, avec des fluctuations modérées, ce qui montre une bonne capacité de généralisation tout en évitant le surapprentissage.\n",
    "- La loss d’entraînement diminue de manière continue et stable, ce qui est attendu avec un entraînement bien maîtrisé.\n",
    "- La loss de validation présente quelques oscillations, mais reste globalement dans une plage basse et ne remonte jamais de façon brutale, ce qui montre que le modèle reste stable et robuste.\n",
    "\n",
    "Le modèle final atteint un excellent compromis entre performance et régularisation, grâce à la combinaison de Dropout, de Data Augmentation et du mécanisme d’Early Stopping. Il affiche des performances comparables aux meilleurs modèles testés, tout en maintenant une stabilité remarquable. Cela en fait un candidat de choix pour une mise en production, notamment pour des données non vues ou bruitées."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "──────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Matrice de confusion avec le modèle final](./results/final_matrix.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyse des résultats :**\n",
    "\n",
    "- Le modèle final parvient à très bien classer les images des classes Photo (1737) et Schematics (1944), avec un très faible taux d’erreur, ce qui montre une bonne robustesse sur ces catégories.\n",
    "- En revanche, on observe une confusion marquée entre les classes Painting et Photo, avec 552 peintures mal classées comme photos, ce qui reste une source importante d’erreurs. Cette confusion est encore plus prononcée qu’avec les versions précédentes du modèle.\n",
    "- La classe Text présente également des confusions notables : 242 images textuelles ont été classées comme Schematics, ce qui peut s’expliquer par la présence de traits ou d’éléments graphiques communs.\n",
    "- Les performances sur la classe Sketch restent stables, avec 247 bonnes prédictions et peu de confusion (seulement 12 erreurs), ce qui montre que le modèle distingue encore bien les traits de style dessinés.\n",
    "\n",
    "Bien que le modèle final reste globalement performant et stable, il semble avoir sacrifié une partie de la précision classe par classe, notamment sur les catégories proches visuellement telles que Painting, Photo et Text. La régularisation a permis d'éviter le surapprentissage, mais elle introduit une hausse des confusions entre classes similaires. Cela reste toutefois un compromis acceptable si l’objectif est la robustesse générale sur des données variées ou bruitées, notamment pour des classes bien séparées comme Schematics ou Photo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. <a id='second-version'>Second version du modèle CNN</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 14.1. <a>Création du modèle</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_2(*args,**kwargs):\n",
    "    activation = 'relu'\n",
    "    model = Sequential(*args, **kwargs)\n",
    "    model.add(layers.Rescaling(scale = 1./255))\n",
    "    model.add(layers.Conv2D(32, (3, 3), padding = 'same', activation = activation))\n",
    "    model.add(layers.ZeroPadding2D(padding = (1, 1)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), padding = 'same', activation = activation))\n",
    "    model.add(layers.ZeroPadding2D(padding = (1, 1)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(128, (3, 3), padding = 'same', activation = activation))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, activation = activation))\n",
    "    model.add(layers.Dropout(0.1))\n",
    "    model.add(layers.Dense(num_classes, activation = 'softmax'))\n",
    "    return model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = create_model_2()\n",
    "model_2.compile(\n",
    "    optimizer = keras.optimizers.Adam(learning_rate = 0.001),\n",
    "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits = False),\n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A l'aide de la bibliothèque \"visualkeras\", nous avons pu réaliser un schéma représentant le modèle que nous avons créé.\n",
    "\n",
    "![modele](./model_imgs/second_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "──────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 14.2. <a>Entraînement du modèle</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model_2 = train_model(model_2, train_set = augmented_train_set, test_set = test_set, epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_matrix(train_model_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style=\"color:yellow;\">TODO</b>\n",
    "\n",
    "Afficher les résultats et les analyser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. <a id='binaire'>Modèle de classification binaire </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette section, nous allons créer un modèle permettant de faire de la classification *binaire*, contrairement à la classification multi-classes que nous avons privilégié jusqu'à présent. Nous utiliserons la même architecture de réseau, mais nous réduirons le jeu de données à deux classes afin d’évaluer l’impact que peut avoir le nombre de classes sur la précision, la complexité du modèle ainsi que d'autres métriques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15.1. <a>Création du modèle binaire</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_binary, test_set_binary = keras.utils.image_dataset_from_directory(\n",
    "    dataset_directory,\n",
    "    label_mode = \"int\",\n",
    "    batch_size = batch_s,\n",
    "    image_size = (image_h, image_w),\n",
    "    seed = 42,\n",
    "    validation_split = 0.2,\n",
    "    subset = \"both\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photo_index = class_names.index(\"Photo\")\n",
    "\n",
    "def convert_label_to_binary(image, label):\n",
    "    return image, tf.cast(tf.equal(label, photo_index), tf.int32)\n",
    "\n",
    "train_set_binary = train_set_binary.map(convert_label_to_binary)\n",
    "test_set_binary = test_set_binary.map(convert_label_to_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not gpus:\n",
    "    train_set_binary = train_set_binary.cache().shuffle(1000).prefetch(buffer_size = AUTOTUNE)\n",
    "    test_set_binary = test_set_binary.cache().prefetch(buffer_size = AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model = Sequential()\n",
    "\n",
    "binary_model.add(layers.Rescaling(1./255))\n",
    "binary_model.add(layers.Conv2D(16, (3, 3), padding = 'same', activation = 'relu'))\n",
    "binary_model.add(layers.MaxPooling2D((2, 2)))\n",
    "binary_model.add(layers.Dropout(0.25))\n",
    "binary_model.add(layers.Conv2D(32, (3, 3), padding = 'same', activation = 'relu'))\n",
    "binary_model.add(layers.Conv2D(64, (3, 3), padding = 'same', activation = 'relu'))\n",
    "binary_model.add(layers.Dropout(0.25))\n",
    "binary_model.add(layers.Flatten())\n",
    "binary_model.add(layers.Dense(128, activation = 'relu'))\n",
    "binary_model.add(layers.Dropout(0.5))\n",
    "binary_model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "binary_model.compile(\n",
    "    optimizer = keras.optimizers.Adam(learning_rate = 0.0001),\n",
    "    loss = keras.losses.BinaryCrossentropy(from_logits = False),\n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A l'aide de la bibliothèque \"visualkeras\", nous avons pu réaliser un schéma représentant le modèle que nous avons créé.\n",
    "\n",
    "![modele](./model_imgs/binary.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_weights = compute_class_weight(\n",
    "    class_weight = \"balanced\",\n",
    "    classes = np.array([0, 1]),\n",
    "    y = np.array([label.numpy() for image, label in train_set_binary.unbatch()])\n",
    ")\n",
    "\n",
    "binary_weights_dict = {0: binary_weights[0], 1: binary_weights[1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_binary = []\n",
    "y_true_binary = []\n",
    "\n",
    "for images, labels in test_set_binary:\n",
    "    X_test_binary.append(images)\n",
    "    y_true_binary.append(labels)\n",
    "\n",
    "X_test_binary = np.concatenate(X_test_binary)\n",
    "y_true_binary = np.concatenate(y_true_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_train_set_binary = train_set_binary.map(lambda x, y: (data_augmentation(x, training = True), y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "──────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15.2. <a>Entraînement du modèle binaire</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model = train_model(binary_model, train_set = augmented_train_set_binary, test_set = test_set_binary, epochs = 20,weights = binary_weights_dict)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Graphique de performance du modèle de classification binaire](./results/binary_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L’accuracy d’entraînement progresse lentement mais régulièrement, atteignant environ 82% à la fin de l’entraînement, ce qui montre que le modèle continue d’apprendre sans surajuster excessivement.\n",
    "- L’accuracy de validation reste systématiquement supérieure à celle d’entraînement, oscillant autour de 84%, avec une bonne stabilité et très peu de chute, ce qui suggère un comportement robuste et une bonne généralisation.\n",
    "- La loss d’entraînement diminue de manière progressive et régulière, traduisant une optimisation continue.\n",
    "- La loss de validation reste globalement stable, avec de légères fluctuations mais sans hausse marquée, ce qui est rassurant sur le plan du surapprentissage.\n",
    "\n",
    "Ce modèle binaire montre un comportement très équilibré entre apprentissage et généralisation. L’écart constant entre les performances sur le jeu d’entraînement et celles sur le jeu de validation peut être interprété comme un effet bénéfique de la régularisation (data augmentation, dropout, et potentiellement le choix d’un taux d’apprentissage plus faible). Bien que l’accuracy n’atteigne pas des sommets, la stabilité globale et l’absence de surapprentissage en font un modèle fiable pour des données nouvelles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "──────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_binary_matrix(model, class_names, X_test = X_test_binary, y_true = y_true_binary):\n",
    "    y_pred_proba = model.predict(X_test)\n",
    "    y_pred = (y_pred_proba.flatten() > 0.5).astype(int)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    display = ConfusionMatrixDisplay(cm, display_labels = class_names)\n",
    "    display.plot(cmap = plt.cm.Blues)\n",
    "    plt.title(\"Matrice de confusion\")\n",
    "    plt.xticks(rotation = 45)\n",
    "    plt.show()\n",
    "    \n",
    "display_binary_matrix(binary_model, ['Non-photo', 'Photo'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Matrice de confusion du modèle binaire](./results/binary_matrix.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Le modèle parvient à correctement identifier 5633 images non-photo et 1338 images photo, ce qui montre une bonne capacité de distinction générale entre les deux catégories.\n",
    "- Il existe toutefois un nombre non négligeable de faux positifs (623), c’est-à-dire des images non-photo classées à tort comme des photos.\n",
    "- De même, on observe 685 faux négatifs, c’est-à-dire des photos classées comme non-photos, ce qui montre que le modèle a encore des difficultés à bien cerner les frontières entre les deux classes.\n",
    "\n",
    "La matrice de confusion confirme les observations faites sur les courbes d’apprentissage : le modèle binaire est globalement performant, mais encore perfectible dans sa capacité à reconnaître les photos, avec un léger déséquilibre dans les erreurs. Ce comportement peut être lié à des similarités visuelles entre certaines images non-photo (comme des peintures ou schémas réalistes) et les vraies photos. Le modèle reste cependant précis, stable et bien régularisé, ce qui en fait un bon candidat pour un cas d’usage où une certaine tolérance aux erreurs est acceptée."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16. <a id='comparaison'>Comparatif des modèles</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 16.1. <a>Créations et entraînements simultanés des modèles</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin de comparer les modèles plus efficacement, nous avons relier les différentes créations ainsi que les entrainements de ces modèles pour permettre de les éxécuter à la suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best_hps\n",
    "train_size = int(0.8 * len(train_set))  \n",
    "val_size = len(train_set) - train_size  \n",
    "train_dataset = train_set.take(train_size)\n",
    "val_dataset = train_set.skip(val_size)\n",
    "tuner.search(\n",
    "    train_dataset,\n",
    "    validation_data = val_dataset,\n",
    "    epochs = 50,\n",
    "    validation_split = 0.2,\n",
    "    callbacks = [stop_early]\n",
    ") \n",
    "\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE MODELS\n",
    "basic_model = create_model()\n",
    "dropout_model = create_model(use_dropout = True)\n",
    "data_augmentation_model = create_model(use_dropout = True)\n",
    "hyperband_model = create_model(hparams = best_hps.values)\n",
    "first_model = create_model(use_dropout = True, hparams = best_hps.values)\n",
    "\n",
    "second_model = create_model_2()\n",
    "\n",
    "second_model.compile(\n",
    "    optimizer = keras.optimizers.Adam(learning_rate = 0.001),\n",
    "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits = False),\n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN MODELS\n",
    "use_hyperparameters = True\n",
    "\n",
    "basic_model = train_model(basic_model, train_set = train_set, test_set = test_set, epochs = 20, Name=\"basic_model\")  \n",
    "basic_model.save(\"models/basic_model.keras\")\n",
    "\n",
    "dropout_model = train_model(dropout_model, train_set = train_set, test_set = test_set, epochs = 20,Name=\"dropout_model\") \n",
    "basic_model.save(\"models/dropout_model.keras\")\n",
    "\n",
    "data_augmentation_model = train_model(data_augmentation_model, train_set = augmented_train_set, test_set = test_set, epochs = 20,Name=\"data_augmentation_model\")\n",
    "basic_model.save(\"models/data_augmentation_model.keras\")\n",
    "\n",
    "hyperband_model = train_model(hyperband_model, train_set = train_set, test_set = test_set, epochs = 20, use_hyperparameters = use_hyperparameters, Name=\"hyperband_model\")\n",
    "hyperband_model.save(\"models/hyperband_model.keras\")\n",
    "\n",
    "first_model = train_model(first_model, train_set = augmented_train_set, test_set = test_set, epochs = 20, use_hyperparameters = use_hyperparameters,  Name=\"first_model\")\n",
    "first_model.save(\"models/first_model.keras\")\n",
    "\n",
    "second_model = train_model(second_model, train_set = augmented_train_set, test_set = test_set, epochs = 20, use_hyperparameters = use_hyperparameters,  Name=\"second_model\")\n",
    "second_model.save(\"models/second_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_model = load_model(\"models/basic_model.keras\")\n",
    "dropout_model = load_model(\"models/dropout_model.keras\")\n",
    "data_augmentation_model = load_model(\"models/data_augmentation_model.keras\")\n",
    "hyperband_model = load_model(\"models/hyperband_model.keras\")\n",
    "first_model = load_model(\"models/first_model.keras\")\n",
    "second_model = load_model(\"models/second_model.keras\")\n",
    "\n",
    "visualkeras.layered_view(\n",
    "    basic_model,\n",
    "    legend = True,\n",
    "    show_dimension = True,\n",
    ")\n",
    "\n",
    "visualkeras.layered_view(\n",
    "    dropout_model,\n",
    "    legend = True,\n",
    "    show_dimension = True,\n",
    ")\n",
    "\n",
    "visualkeras.layered_view(\n",
    "    data_augmentation_model,\n",
    "    legend = True,\n",
    "    show_dimension = True,\n",
    ")\n",
    "\n",
    "visualkeras.layered_view(\n",
    "    hyperband_model,\n",
    "    legend = True,\n",
    "    show_dimension = True,\n",
    ")\n",
    "\n",
    "visualkeras.layered_view(\n",
    "    first_model,\n",
    "    legend = True,\n",
    "    show_dimension = True,\n",
    ")\n",
    "\n",
    "visualkeras.layered_view(\n",
    "    second_model,\n",
    "    legend = True,\n",
    "    show_dimension = True,\n",
    ")\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "──────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 16.2. <a>Génération des graphiques</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons ensuite créer un code permettant de récupérer tout les différents csv stockés dans le répertoire results_csv, et de générer un graphique avec les stats d'accuracy et de loss obtenus au cours de l'entrainement des différents modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder containing the CSV files\n",
    "folder = \"results_csv\"\n",
    "\n",
    "# Initialize lists to store data for plotting\n",
    "model_names = []\n",
    "dataframes = []\n",
    "\n",
    "# Iterate through CSV files in the folder\n",
    "for file in os.listdir(folder):\n",
    "  if file.endswith(\".csv\"):\n",
    "    # Extract model name (file name without extension)\n",
    "    model_name = os.path.splitext(file)[0]\n",
    "    model_names.append(model_name)\n",
    "    \n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(os.path.join(folder, file), index_col=0)\n",
    "    dataframes.append((model_name, df))\n",
    "\n",
    "# Generate a color map for models\n",
    "colors = plt.cm.tab10(range(len(model_names)))\n",
    "\n",
    "# Plot accuracy and loss\n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "# Subplot for accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "for (model_name, df), color in zip(dataframes, colors):\n",
    "  plt.plot(df['accuracy'], label=f'{model_name} - Training Accuracy', color=color)\n",
    "  plt.plot(df['val_accuracy'], linestyle='--', label=f'{model_name} - Validation Accuracy', color=color)\n",
    "plt.legend(loc='lower right')\n",
    "plt.title(\"Training and Validation Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "\n",
    "# Subplot for loss\n",
    "plt.subplot(1, 2, 2)\n",
    "for (model_name, df), color in zip(dataframes, colors):\n",
    "  plt.plot(df['loss'], label=f'{model_name} - Training Loss', color=color)\n",
    "  plt.plot(df['val_loss'], linestyle='--', label=f'{model_name} - Validation Loss', color=color)\n",
    "plt.legend(loc='upper right')\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "# Show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Accuracy des différents models finis dans un schéma](./results/all_finish_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons observer sur ce schéma la comparaison entre nos 3 modèles principaux : les 2 modèles en CNN ainsi que le modèle binaire. A première vue, le modèle binaire semble correspondre à notre demander en terme de résultat obtenant une accuracy d'environ 0.80 et une loss d'environ 0.35. Cependant, ces résultats restent en dessous de ceux des deux autres modèles qui, eux, s'approchent fortemment en terme de résultat. Selon les résultats finaux, le premier modèle semble plus interessant mais leur ressemblance étant importante, il va être encore nécéssaire de tester leur fonctionnement avec un jeu de test afin de vérifier notre théorie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Accuracy des différents models finis dans un schéma](./results/sub_model_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous observons dans le schéma ci-dessus les résultats de tous les modèles secondaires, c'est-à-dire les modèles ne contenant qu'une des opérations d'amélioration que nous avons imaginées. Cela nous permet d'observer que l'ajout des hyperparamètres semble être le paramètre nous permettant la meilleure amélioration. Cependant, ayant appliqué l'early stopping à chacun des modèles, nous pouvons observer qu'une majorité d'entre eux s'arrête avant la fin des epochs dû à un overfitting arrivant très vite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17. <a id='metrics'>Calcul des métriques de performance</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction suivante calcul différentes métriques comme la précision, l'accuracy, le recall, le $\\text{R}^2$ ou le F1 score.\n",
    "Elle permet de facilement obtenir les différentes métriques pour ensuite les comparer entre les différents modèles.\n",
    "\n",
    "#### Rappel des Formules :  \n",
    "\n",
    "les formules utilisent les termes suivant,  \n",
    "$\\text{TP = Vrai positif}\\\\\n",
    "\\text{TN = Vrai négatif}\\\\\n",
    "\\text{FP = Faux positif}\\\\\n",
    "\\text{FN = Faux négatif}$\n",
    "\n",
    "Accuracy :  $\\frac{TP+TN}{TP+TN+FP+FN}$\n",
    "\n",
    "Précision :  $\\frac{TP}{TP+TN}$\n",
    "\n",
    "Recall :  $\\frac{TP}{TP+FN}$\n",
    "\n",
    "F1 Score :  $2 \\times \\frac{\\text{précision} \\times \\text{recall} }{\\text{précision} + \\text{recall}}$\n",
    "\n",
    "$\\text{R}^2$ :  $ 1- \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y}_i)^2} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculMetrics(model, test_set):\n",
    "    y_true = []  \n",
    "    y_pred = []  \n",
    "    y_true_2 = []  \n",
    "    y_pred_2 = []  \n",
    "\n",
    "    for (images, labels) in test_set:\n",
    "        predictions = model.predict(images, verbose=0)  \n",
    "        y_true.extend(labels.numpy()) \n",
    "        y_pred.extend(predictions.argmax(axis=1))\n",
    "        y_true_2.extend(labels.numpy())\n",
    "        y_pred_2.extend(predictions.argmax(axis=1))\n",
    "\n",
    "    # Convertissez y_true, y_pred et all_images en tableaux NumPy pour une manipulation plus facile\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_true_2 = np.array(y_true_2)\n",
    "    y_pred_2 = np.array(y_pred_2)\n",
    "\n",
    "    # le f1 score et le r2 score ont besoin d'être retournés pour fonctionner (exemple : [1, 2] -> [[1], [2]])\n",
    "    y_pred_2 = y_pred_2.reshape(-1, 1)\n",
    "    y_true_2 = y_true_2.reshape(-1, 1)\n",
    "\n",
    "    # si on ne réimporte pas on as une erreur quand on relance la cellule\n",
    "    from tensorflow.keras.metrics import Precision, Recall, Accuracy\n",
    "\n",
    "    Precision = Precision()\n",
    "    Recall = Recall()\n",
    "    Precision.update_state(y_true, y_pred)\n",
    "    Recall.update_state(y_true, y_pred)\n",
    "    Accuracy = Accuracy()\n",
    "    R2Score = tf.keras.metrics.R2Score()\n",
    "    Accuracy.update_state(y_true, y_pred)\n",
    "    R2Score.update_state(y_true_2, y_pred_2)\n",
    "    F1Score = tf.metrics.F1Score()\n",
    "    F1Score.update_state(y_true_2, y_pred_2)\n",
    "    return {\"Precision\": Precision.result(), \"Recall\": Recall.result(), \"Accuracy\": Accuracy.result(), \"R2Score\": R2Score.result(), \"F1Score\": F1Score.result()[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "# chargement des modèles\n",
    "for model in os.listdir(\"models\"):\n",
    "    model_path = os.path.join(\"models\", model)\n",
    "    if model.endswith(\".keras\"):\n",
    "        loaded_model = load_model(model_path)\n",
    "        # calcul des métriques\n",
    "        values = calculMetrics(loaded_model, test_set)\n",
    "        # ajout du nom du modèle\n",
    "        values[\"Name\"] = model_path.split(\"\\\\\")[-1].split(\".\")[0]\n",
    "        metrics.append(values)\n",
    "        print(f\"{model_path} done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = {}\n",
    "# On doit regrouper les métriques ensemble pour les afficher sur le même graphique\n",
    "for metric in metrics:\n",
    "    for j, key in enumerate(metric.keys()):\n",
    "        if val.get(key) is None:\n",
    "            val[key] = []\n",
    "        val[key].append(metric[key])\n",
    "\n",
    "# Affichages des graphiques en barres\n",
    "plt.figure(figsize=(10, 10))\n",
    "for index, key in enumerate(val.keys()):\n",
    "    if key != \"Name\":\n",
    "        plt.subplot(3, 2, index + 1)\n",
    "        plt.bar(val[\"Name\"], val[key])\n",
    "        plt.title(key)\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style=\"color:yellow;\">TODO / explication résultat</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18. <a id='conclusion'>Conclusion</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style=\"color:yellow;\">TODO</b>\n",
    "\n",
    "Écrire la conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
