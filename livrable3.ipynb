{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> **Livrable n°3 : Captioning d'image** </center>\n",
    "\n",
    "‎ \n",
    "\n",
    "Réalisé par le **groupe n°2** :\n",
    "- BERTHO Lucien\n",
    "- BOSACKI Paul\n",
    "- GAURE Warren\n",
    "- GRENOUILLET Théo\n",
    "- VALLEMONT Hugo\n",
    "\n",
    "\n",
    "‎\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Sommaire**\n",
    "\n",
    "1. [Mise en contexte](#contexte)\n",
    "2. [Objectif du livrable](#objectif)\n",
    "3. [Importation des bibliothèques](#import)\n",
    "4. [Prétraitement et exploration des données](#pretraitement)\n",
    "5. [Item #5](#item5)\n",
    "6. [Item #6](#item6)\n",
    "7. [Item #7](#item7)\n",
    "8. [Item #8](#item8)\n",
    "9. [Item #9](#item9)\n",
    "10. [Item #10](#item10)\n",
    "\n",
    "‎ \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. <a id='contexte'>Mise en contexte</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L’entreprise TouNum est spécialisée dans la numérisation de documents, qu’il s’agisse de textes ou d’images. Ses services sont particulièrement sollicités par des entreprises cherchant à transformer leur base documentaire papier en fichiers numériques exploitables. Aujourd’hui, TouNum souhaite aller plus loin en enrichissant son offre avec des outils basés sur le Machine Learning.\n",
    "\n",
    "En effet, certains clients disposent d’un volume considérable de documents à numériser et expriment un besoin croissant pour des solutions de catégorisation automatique. Une telle innovation leur permettrait d’optimiser le traitement et l’exploitation de leurs données numérisées. Toutefois, TouNum ne dispose pas en interne des compétences nécessaires pour concevoir et mettre en place ces technologies.\n",
    "\n",
    "C’est dans ce cadre que notre équipe de spécialistes en Data Science du CESI est sollicitée. Notre mission consiste à développer une première solution intégrant du captioning automatique : un système capable d’analyser des photographies et de générer une légende descriptive de manière autonome.\n",
    "\n",
    "Heureusement, TouNum possède déjà plusieurs milliers d’images annotées, ce qui constituera une ressource précieuse pour entraîner les modèles de Machine Learning à partir d’un apprentissage supervisé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. <a id='objectif'>Objectif du livrable</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. <a id='import'>Importation des bibliothèques</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import collections\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            details = tf.config.experimental.get_device_details(gpu)\n",
    "            print(f\"Nom du GPU détecté : {details.get('device_name', 'Nom inconnu')}\")\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. <a id='pretraitement'>Prétraitement et exploration des données</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois les bibliothèques importées, nous pouvons désormais gérer les données qui nous ont été données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1. <a>Vérification des images</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_directory = \"dataset_livrable_1/Photo/\"\n",
    "dataset_directory2 = \"dataset_livrable_2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def is_valid_image(path):\n",
    "#    try:\n",
    "#        img_raw = tf.io.read_file(path)\n",
    "#        _ = tf.image.decode_image(img_raw, channels=3)\n",
    "#        return (path, True)\n",
    "#    except Exception:\n",
    "#        return (path, False)\n",
    "#\n",
    "#def clean_corrupted_images(directory, extensions=(\"jpg\", \"jpeg\", \"png\"), max_workers=8):\n",
    "#    image_paths = []\n",
    "#    for root, _, files in os.walk(directory):\n",
    "#        for file in files:\n",
    "#            if file.lower().endswith(extensions):\n",
    "#                image_paths.append(os.path.join(root, file))\n",
    "#\n",
    "#    print(f\"Scan de {len(image_paths)} images dans {directory}\")\n",
    "#\n",
    "#    corrupted_count = 0\n",
    "#    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "#        futures = [executor.submit(is_valid_image, path) for path in image_paths]\n",
    "#        for future in as_completed(futures):\n",
    "#            path, is_valid = future.result()\n",
    "#            if not is_valid:\n",
    "#                try:\n",
    "#                    os.remove(path)\n",
    "#                    corrupted_count += 1\n",
    "#                except Exception as e:\n",
    "#                    print(f\"Erreur de suppression {path} : {e}\")\n",
    "#\n",
    "#    print(f\"Vérification terminée : {corrupted_count} image(s) corrompue(s) supprimée(s).\")\n",
    "#    \n",
    "#\n",
    "#clean_corrupted_images(dataset_directory)\n",
    "#clean_corrupted_images(dataset_directory2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2. <a>Chargement des données</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_h = 128\n",
    "image_w = 128\n",
    "batch_s = 16\n",
    "\n",
    "dataset = keras.utils.image_dataset_from_directory(\n",
    "    directory = dataset_directory,\n",
    "    label_mode = None,\n",
    "    batch_size = batch_s,\n",
    "    image_size = (image_h, image_w),\n",
    "    seed = 42\n",
    ")\n",
    "\n",
    "dataset_2 = keras.utils.image_dataset_from_directory(\n",
    "    directory = dataset_directory2,\n",
    "    label_mode = None,\n",
    "    batch_size = batch_s,\n",
    "    image_size = (image_h, image_w),\n",
    "    seed = 42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3. <a>Prétraitement des données</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Étant donné que nous travaillons à nouveau avec des images, c'est-à-dire des ensembles de pixels dont les valeurs sont comprises entre 0 et 255, nous devons les mettre à dans l'intervalle `[0, 1]` afin de mieux les traiter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda x: tf.cast(x, tf.float32) / 255.0)\n",
    "dataset_2 = dataset_2.map(lambda x: tf.cast(x, tf.float32) / 255.0)\n",
    "X = []\n",
    "\n",
    "for batch in dataset:\n",
    "    X.append(batch.numpy())\n",
    "\n",
    "for batch in dataset_2:\n",
    "    X.append(batch.numpy())\n",
    "    \n",
    "dataset = np.concatenate(X)\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4. <a>Affichage des images</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que le prétraitement est fait, nous pouvons afficher des images grâce à une fonction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images():\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    for i in range(16):\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        plt.imshow(dataset[i])\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dossier d'annotation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ProjectPath = 'C:\\\\DataScience\\\\Project\\\\'\n",
    "\n",
    "# Chemin du fichier d'annotations\n",
    "annotation_file = f\"{ProjectPath}dataset_livrable_3\\\\annotations\\\\captions_train2014.json\"\n",
    "\n",
    "# Chemin du dossier contenant les images à annoter\n",
    "PATH = f\"{ProjectPath}dataset_livrable_3\\\\train2014\\\\train2014\\\\\"\n",
    "PATH2 = f\"{ProjectPath}dataset_livrable_2\\\\Dataset\\\\\"\n",
    "\n",
    "\n",
    "# Lecture du fichier d'annotation\n",
    "with open(annotation_file, 'r') as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "# Grouper toutes les annotations ayant le meme identifiant.\n",
    "image_path_to_caption = collections.defaultdict(list)\n",
    "for val in annotations['annotations']:\n",
    "    # marquer le debut et la fin de chaq0+ ue annotation\n",
    "    caption = '<start> ' + val['caption'] + ' <end>'\n",
    "    # L'identifiant d'une image fait partie de son chemin d'accès\n",
    "    image_path = PATH + 'COCO_train2014_' + '%012d.jpg' % (val['image_id'])\n",
    "    #image_path = PATH2 + 'noisy_' + '%03d.jpg' % (val['image_id'])\n",
    "    # Rajout du caption associé à image_path\n",
    "    image_path_to_caption[image_path].append(caption)\n",
    "\n",
    "# Print the first 10 values of image_path_to_caption\n",
    "#for i, (key, value) in enumerate(image_path_to_caption.items()):\n",
    "#    if os.path.exists(key):\n",
    "#        if i == 10:\n",
    "#            break\n",
    "#        print(f\"{key}: {value}\")\n",
    "#        img = Image.open(key)\n",
    "#        plt.imshow(img)\n",
    "#        plt.axis(\"off\")\n",
    "#        plt.show()\n",
    "    \n",
    "# Prendre les premières images seulement\n",
    "image_paths = list(image_path_to_caption.keys())\n",
    "train_image_paths = image_paths[:2000]\n",
    "\n",
    "# Liste de toutes les annotations\n",
    "train_captions = []\n",
    "# Liste de tous les noms de fichiers des images dupliquées (en nombre d'annotations par image)\n",
    "img_name_vector = []\n",
    "\n",
    "for image_path in train_image_paths:\n",
    "    caption_list = image_path_to_caption[image_path]\n",
    "    # Rajout de caption_list dans train_captions\n",
    "    train_captions.extend(caption_list)\n",
    "    # Rajout de image_path dupliquée len(caption_list) fois\n",
    "    img_name_vector.extend([image_path] * len(caption_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Telechargement du modèle InceptionV3 pré-entrainé avec la cassification sur ImageNet\n",
    "image_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n",
    "# Creation d'une variable qui sera l'entrée du nouveau modèle de pre-traitement d'images\n",
    "new_input = image_model.input\n",
    "# récupérer la dernière couche caché qui contient l'image en representation compacte\n",
    "hidden_layer = image_model.layers[-1].output\n",
    "\n",
    "# Modèle qui calcule une representation dense des images avec InceptionV3\n",
    "image_features_extract_model = tf.keras.Model(inputs = new_input, outputs = hidden_layer)\n",
    "\n",
    "# Définition de la fonction load_image\n",
    "def load_image(image_path):\n",
    "    \"\"\"\n",
    "    La fonction load_image a pour entrée le chemin d'une image et pour sortie un couple\n",
    "    contenant l'image traitée ainsi que son chemin d'accès.\n",
    "    La fonction load_image effectue les traitement suivant:\n",
    "        1. Chargement du fichier correspondant au chemin d'accès image_path\n",
    "        2. Décodage de l'image en RGB.\n",
    "        3. Redimensionnement de l'image en taille (299, 299).\n",
    "        4. Normalisation des pîxels de l'image entre -1 et 1\n",
    "    \"\"\"\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (299, 299))\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    return img, image_path\n",
    "\n",
    "# Pré-traitement des images\n",
    "# Prendre les noms des images\n",
    "encode_train = sorted(set(img_name_vector))\n",
    "\n",
    "# Creation d'une instance de \"tf.data.Dataset\" partant des noms des images \n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
    "# Division du données en batchs après application du pré-traitement fait par load_image\n",
    "image_dataset = image_dataset.map(\n",
    "  load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(16)\n",
    "\n",
    "# Parcourir le dataset batch par batch pour effectuez le pré-traitement d'InceptionV3\n",
    "for img, path in tqdm(image_dataset):\n",
    "    # Pré-traitement du batch (de taille (16,8,8,2048)) courant par InceptionV3 \n",
    "    batch_features = image_features_extract_model(img)\n",
    "    # Resize du batch de taille (16,8,8,2048) en taille (16,64,2048)\n",
    "    batch_features = tf.reshape(batch_features,\n",
    "                              (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "    # Parcourir le batch courant et stocker le chemin ainsi que le batch avec np.save()\n",
    "    for bf, p in zip(batch_features, path):\n",
    "        path_of_feature = p.numpy().decode(\"utf-8\")\n",
    "        # (chemin de l'image associe a sa nouvelle representation , representation de l'image)\n",
    "        np.save(path_of_feature, bf.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trouver la taille maximale \n",
    "def calc_max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n",
    "\n",
    "# Chosir les 5000 mots les plus frequents du vocabulaire\n",
    "top_k = 5000\n",
    "#La classe Tokenizer permet de faire du pre-traitement de texte pour reseau de neurones \n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
    "                                                  oov_token=\"<unk>\",\n",
    "                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "# Construit un vocabulaire en se basant sur la liste train_captions\n",
    "tokenizer.fit_on_texts(train_captions)\n",
    "    \n",
    "# Créer le token qui sert à remplir les annotations pour egaliser leurs longueur\n",
    "tokenizer.word_index['<pad>'] = 0\n",
    "tokenizer.index_word[0] = '<pad>'\n",
    "\n",
    "# Creation des vecteurs(liste de token entiers) à partir des annotations (liste de mots)\n",
    "train_seqs = tokenizer.texts_to_sequences(train_captions)\n",
    "\n",
    "# Remplir chaque vecteur à jusqu'à la longueur maximale des annotations\n",
    "cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')\n",
    "\n",
    "# Calcule la longueur maximale qui est utilisée pour stocker les poids d'attention \n",
    "# Elle servira plus tard pour l'affichage lors de l'évaluation\n",
    "max_length = calc_max_length(train_seqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. <a>Modèle de classification</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_to_cap_vector = collections.defaultdict(list)\n",
    "# Creation d'un dictionnaire associant les chemins des images avec (fichier .npy) aux annotationss\n",
    "# Les images sont dupliquées car il y a plusieurs annotations par image\n",
    "print(len(img_name_vector), len(cap_vector))\n",
    "for img, cap in zip(img_name_vector, cap_vector):\n",
    "    img_to_cap_vector[img].append(cap)\n",
    "\n",
    "\"\"\"\n",
    "Création des datasets de formation et de validation en utilisant \n",
    "un fractionnement 80-20 de manière aléatoire\n",
    "\"\"\" \n",
    "# Prendre les clés (noms des fichiers d'images traites), *celles-ci ne seront pas dupliquées*\n",
    "img_keys = list(img_to_cap_vector.keys())\n",
    "# Diviser des indices en entrainement et test\n",
    "slice_index = int(len(img_keys) * 0.8)\n",
    "img_name_train_keys = img_keys[:slice_index]\n",
    "img_name_val_keys = img_keys[slice_index:]\n",
    "\n",
    "\"\"\"\n",
    "Les jeux d'entrainement et de tests sont sous forme\n",
    "de listes contenants les mappings :(image prétraitée ---> jeton d'annotation(mot) )\n",
    "\"\"\"\n",
    "\n",
    "# Boucle pour construire le jeu d'entrainement\n",
    "img_name_train = []\n",
    "cap_train = []\n",
    "for imgt in img_name_train_keys:\n",
    "    capt_len = len(img_to_cap_vector[imgt])\n",
    "    # Duplication des images en le nombre d'annotations par image\n",
    "    img_name_train.extend([imgt] * capt_len)\n",
    "    cap_train.extend(img_to_cap_vector[imgt])\n",
    "\n",
    "# Boucle pour construire le jeu de test\n",
    "img_name_val = []\n",
    "cap_val = []\n",
    "for imgv in img_name_val_keys:\n",
    "    capv_len = len(img_to_cap_vector[imgv])\n",
    "    img_name_val.extend([imgv] * capv_len)\n",
    "    cap_val.extend(img_to_cap_vector[imgv])\n",
    "\n",
    "len(img_name_train), len(cap_train), len(img_name_val), len(cap_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N'hésitez pas à modifier ces paramètres en fonction de votre machine\n",
    "BATCH_SIZE = 64 # taille du batch\n",
    "BUFFER_SIZE = 1000 # taille du buffer pour melanger les donnes\n",
    "embedding_dim = 256\n",
    "units = 512 # Taille de la couche caché dans le RNN\n",
    "vocab_size = top_k + 1\n",
    "num_steps = len(img_name_train) // BATCH_SIZE\n",
    "\n",
    "# La forme du vecteur extrait à partir d'InceptionV3 est (64, 2048)\n",
    "# Les deux variables suivantes representent la forme de ce vecteur\n",
    "features_shape = 2048\n",
    "attention_features_shape = 64\n",
    "\n",
    "# Fonction qui charge les fichiers numpy des images prétraitées\n",
    "def map_func(img_name, cap):\n",
    "    img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
    "    return img_tensor, cap\n",
    "\n",
    "# Creation d'un dataset de \"Tensor\"s (sert à representer de grands dataset)\n",
    "# Le dataset est cree a partir de \"img_name_train\" et \"cap_train\"\n",
    "dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n",
    "\n",
    "# L'utilisation de map permet de charger les fichiers numpy (possiblement en parallèle)\n",
    "dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
    "          map_func, [item1, item2], [tf.float32, tf.int32]),\n",
    "          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Melanger les donnees et les diviser en batchs\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Encoder(tf.keras.Model):\n",
    "    # Comme les images sont déjà prétraités par InceptionV3 est représenté sous forme compacte\n",
    "    # L'encodeur CNN ne fera que transmettre ces caractéristiques à une couche dense\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        # forme après fc == (batch_size, 64, embedding_dim)\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decodeur/Encodeur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        # features(CNN_encoder output) forme == (batch_size, 64, embedding_dim)\n",
    "\n",
    "        # forme de la couche cachée == (batch_size, hidden_size)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "        attention_hidden_layer = tf.nn.tanh(\n",
    "                self.W1(features) + self.W2(hidden_with_time_axis)\n",
    "        )\n",
    "\n",
    "        # Cela vous donne un score non normalisé pour chaque caractéristique de l'image.\n",
    "        score = self.V(attention_hidden_layer)\n",
    "\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        \n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Decoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size, use_lstm=False):\n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.use_lstm = use_lstm\n",
    "        if not use_lstm:\n",
    "            self.layer = tf.keras.layers.GRU(\n",
    "                self.units,\n",
    "                return_sequences=True,\n",
    "                return_state=True,\n",
    "                #recurrent_initializer='glorot_uniform',\n",
    "                \n",
    "                activation='tanh',\n",
    "                recurrent_activation='sigmoid',\n",
    "                use_bias=True,\n",
    "                kernel_initializer='glorot_uniform',\n",
    "                recurrent_initializer='orthogonal',\n",
    "                bias_initializer='zeros',\n",
    "                unroll=True,\n",
    "            # kernel_regularizer=None,\n",
    "            # recurrent_regularizer=None,\n",
    "            # bias_regularizer=None,\n",
    "            # activity_regularizer=None,\n",
    "            # kernel_constraint=None,\n",
    "            # recurrent_constraint=None,\n",
    "            # bias_constraint=None,\n",
    "            # dropout=0.0,\n",
    "            # recurrent_dropout=0.0,\n",
    "            # seed=None,\n",
    "            # go_backwards=True,\n",
    "            # stateful=True,\n",
    "            # reset_after=True,\n",
    "            # use_cudnn='auto'\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            self.layer = tf.keras.layers.LSTM(\n",
    "                self.units,\n",
    "                return_sequences=True,\n",
    "                return_state=True,\n",
    "                #recurrent_initializer='glorot_uniform',\n",
    "                \n",
    "                activation='tanh',\n",
    "                recurrent_activation='sigmoid',\n",
    "                use_bias=True,\n",
    "                kernel_initializer='glorot_uniform',\n",
    "                recurrent_initializer='orthogonal',\n",
    "                bias_initializer='zeros',\n",
    "                unroll=True,\n",
    "                # kernel_regularizer=None,\n",
    "                # recurrent_regularizer=None,\n",
    "                # bias_regularizer=None,\n",
    "                # activity_regularizer=None,\n",
    "                # kernel_constraint=None,\n",
    "                # recurrent_constraint=None,\n",
    "                # bias_constraint=None,\n",
    "                # dropout=0.0,\n",
    "                # recurrent_dropout=0.0,\n",
    "                # seed=None,\n",
    "                # go_backwards=True,\n",
    "                # stateful=True,\n",
    "                # reset_after=True\n",
    "            )\n",
    "\n",
    "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "\n",
    "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "    def call(self, x, features, hidden):\n",
    "        # features: sortie de l'encodeur CNN (batch_size, 64, embedding_dim)\n",
    "        # hidden: état caché précédent du GRU (batch_size, units)\n",
    "        # x: mot courant (batch_size, 1)\n",
    "\n",
    "        context_vector, attention_weights = self.attention(features, hidden)\n",
    "\n",
    "        x = self.embedding(x) \n",
    "\n",
    "        context_vector = tf.expand_dims(context_vector, 1)  \n",
    "\n",
    "        x = tf.concat([context_vector, x], axis=-1)  \n",
    "\n",
    "        if not self.use_lstm:\n",
    "            output, state = self.layer(x)\n",
    "        else:\n",
    "            output, state, a = self.layer(x)  \n",
    "\n",
    "        y = self.fc1(output)\n",
    "\n",
    "        y = tf.reshape(y, (-1, y.shape[2]))\n",
    "\n",
    "        y = self.fc2(y)\n",
    "\n",
    "        return y, state, attention_weights\n",
    "\n",
    "    def reset_state(self, batch_size):\n",
    "       return tf.zeros((batch_size, self.units))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 256\n",
    "units = 512\n",
    "vocab_size = top_k + 1\n",
    "\n",
    "# Création de l'encodeur (CNN)\n",
    "encoder = CNN_Encoder(embedding_dim)\n",
    "\n",
    "# Création du décodeur (RNN avec attention)\n",
    "decoderLSTM = RNN_Decoder(embedding_dim, units, vocab_size, use_lstm=True)\n",
    "decoderGRU = RNN_Decoder(embedding_dim, units, vocab_size, use_lstm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimiseur ADAM\n",
    "optimizer_LSTM = tf.keras.optimizers.Adam()\n",
    "optimizer_GRU = tf.keras.optimizers.Adam()\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    \n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_plot_LSTM = []\n",
    "loss_plot_GRU = []\n",
    "#@tf.function\n",
    "def train_step(img_tensor, target, decoder, optimizer):\n",
    "    loss = 0\n",
    "\n",
    "    # Initialisation de l'état caché pour chaque batch\n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "\n",
    "    # Initialiser l'entrée du décodeur\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
    "    \n",
    "    with tf.GradientTape() as tape: # Offre la possibilité de calculer le gradient du loss\n",
    "        features = encoder(img_tensor)\n",
    "\n",
    "        for i in range(1, target.shape[1]):\n",
    "            # Prédiction des i'èmes mot du batch avec le décodeur\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "            # Le mot correct à l'étap i est donné en entrée à l'étape (i+1)\n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "    total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparation pour la sauvegarde du meilleur modèle\n",
    "checkpoint_path = \"./checkpoints/LSTM_model\"\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoderLSTM,\n",
    "                           optimizer=optimizer_LSTM)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=1)\n",
    "\n",
    "# Préparation pour la sauvegarde du meilleur modèle\n",
    "checkpoint_path = \"./checkpoints/GRU_model\"\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoderGRU,\n",
    "                           optimizer=optimizer_GRU)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = []\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor = 'val_accuracy',\n",
    "    patience = 3,\n",
    "    mode = 'max',\n",
    "    restore_best_weights = True\n",
    ")\n",
    "\n",
    "callbacks.append(early_stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "\n",
    "# if ckpt_manager.latest_checkpoint:\n",
    "#     ckpt.restore(ckpt_manager.latest_checkpoint).expect_partial()\n",
    "# else:\n",
    "callbacks.append(early_stopping)\n",
    "best_loss_LSTM = float('inf')\n",
    "best_loss_GRU = float('inf')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss_LSTM = 0\n",
    "    total_loss_GRU = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in enumerate(dataset):\n",
    "        batch_loss_LSTM, t_loss_LSTM = train_step(img_tensor, target, decoderLSTM, optimizer_LSTM)\n",
    "        batch_loss_GRU, t_loss_GRU = train_step(img_tensor, target, decoderGRU, optimizer_GRU)\n",
    "        total_loss_LSTM += t_loss_LSTM\n",
    "        total_loss_GRU += t_loss_GRU\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print('LSTM : Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss_LSTM.numpy() / int(target.shape[1])))\n",
    "            print('GRU : Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss_GRU.numpy() / int(target.shape[1])))\n",
    "\n",
    "    epoch_loss_LSTM = total_loss_LSTM / num_steps\n",
    "    epoch_loss_GRU = total_loss_GRU / num_steps\n",
    "    loss_plot_LSTM.append(epoch_loss_LSTM)\n",
    "    loss_plot_GRU.append(epoch_loss_GRU)\n",
    "\n",
    "    print('LSTM : Epoch {} Batch {} Loss {:.6f}'.format(epoch + 1, batch, epoch_loss_LSTM))\n",
    "    print('GRU : Epoch {} Batch {} Loss {:.6f}'.format(epoch + 1, batch, epoch_loss_GRU))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "\n",
    "    # Sauvegarde du modèle uniquement si la perte est meilleure\n",
    "    if epoch_loss_LSTM < best_loss_LSTM:\n",
    "        best_loss_LSTM = epoch_loss_LSTM\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "\n",
    "    if epoch_loss_GRU < best_loss_GRU:\n",
    "        best_loss_GRU = epoch_loss_GRU\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "\n",
    "# Affichage de la courbe d'entrainement\n",
    "plt.plot(loss_plot_LSTM, label='LSTM')\n",
    "plt.plot(loss_plot_GRU, label='GRU')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.save('models/encoder_model.h5')\n",
    "decoderLSTM.save('models/decoder_LSTM_model.h5')\n",
    "decoderGRU.save('models/decoder_GRU_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display attention on the image\n",
    "def plot_attention(image, result, attention_plot):\n",
    "    temp_image = np.array(Image.open(image))\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "    len_result = len(result)\n",
    "    for l in range(len_result):\n",
    "        temp_att = np.resize(attention_plot[l], (8, 8))\n",
    "        ax = fig.add_subplot(len_result // 2, len_result // 2, l + 1)\n",
    "        ax.set_title(result[l])\n",
    "        img = ax.imshow(temp_image)\n",
    "        ax.imshow(temp_att, cmap='gray', \n",
    "        alpha=0.6, extent=img.get_extent())\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(image, decoder):\n",
    "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
    "\n",
    "    hidden = decoder.reset_state(batch_size=1)\n",
    "\n",
    "    temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
    "    img_tensor_val = image_features_extract_model(temp_input)\n",
    "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "\n",
    "    features = encoder(img_tensor_val)\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
    "    result = []\n",
    "\n",
    "    for i in range(max_length):\n",
    "        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
    "\n",
    "        attention_plot[i] = tf.reshape(attention_weights, (-1,)).numpy()\n",
    "\n",
    "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
    "        result.append(tokenizer.index_word[predicted_id])\n",
    "\n",
    "        if tokenizer.index_word[predicted_id] == '<end>':\n",
    "            break\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    attention_plot = attention_plot[:len(result), :]\n",
    "    return result, attention_plot\n",
    "\n",
    "# Une seule image de départ\n",
    "rid = np.random.randint(0, len(img_name_val))\n",
    "image = img_name_val[rid]\n",
    "index = img_name_val.index(image)\n",
    "real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[index] if i not in [0, tokenizer.word_index['<start>'], tokenizer.word_index['<end>']]])\n",
    "\n",
    "# Evaluation répétée jusqu'à obtenir 3 prédictions consécutives identiques\n",
    "previous_captions_LSTM = []\n",
    "previous_captions_GRU = []\n",
    "streak_LSTM = 1\n",
    "streak_GRU = 1\n",
    "last_caption_LSTM = None\n",
    "last_caption_GRU = None\n",
    "\n",
    "while True:\n",
    "    result_LSTM, attention_plot_LSTM = evaluate(image, decoderLSTM)\n",
    "    result_GRU, attention_plot_GRU = evaluate(image, decoderLSTM)\n",
    "    predicted_caption_LSTM = ' '.join([word for word in result_LSTM if word not in ['<start>', '<end>']])\n",
    "    predicted_caption_GRU = ' '.join([word for word in result_GRU if word not in ['<start>', '<end>']])\n",
    "\n",
    "    #print(predicted_caption)\n",
    "    #print(last_caption)\n",
    "\n",
    "    if predicted_caption_LSTM == last_caption_LSTM:\n",
    "        streak_LSTM += 1\n",
    "    else:\n",
    "        streak_LSTM = 1\n",
    "        last_caption_LSTM = predicted_caption_LSTM\n",
    "\n",
    "    if streak_LSTM == 2:\n",
    "        print('Image Path:', image)\n",
    "        print('Real Caption:', real_caption)\n",
    "        print('Predicted Caption:', predicted_caption_LSTM)\n",
    "        plot_attention(image, result_LSTM, attention_plot_LSTM)\n",
    "        break\n",
    "\n",
    "    if predicted_caption_GRU == last_caption_GRU:\n",
    "        streak_GRU += 1\n",
    "    else:\n",
    "        streak_GRU = 1\n",
    "        last_caption_GRU = predicted_caption_GRU\n",
    "\n",
    "    if streak_GRU == 2:\n",
    "        print('Image Path:', image)\n",
    "        print('Real Caption:', real_caption)\n",
    "        print('Predicted Caption:', predicted_caption_GRU)\n",
    "        plot_attention(image, result_GRU, attention_plot_GRU)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def evaluate(image):\n",
    "#    attention_plot = np.zeros((max_length, attention_features_shape))\n",
    "#\n",
    "#    hidden = decoder.reset_state(batch_size=1)\n",
    "#\n",
    "#    temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
    "#    img_tensor_val = image_features_extract_model(temp_input)\n",
    "#    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "#\n",
    "#    features = encoder(img_tensor_val)\n",
    "#\n",
    "#    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
    "#    result = []\n",
    "#\n",
    "#    for i in range(max_length):\n",
    "#        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
    "#\n",
    "#        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "#\n",
    "#        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
    "#        result.append(tokenizer.index_word[predicted_id])\n",
    "#\n",
    "#        if tokenizer.index_word[predicted_id] == '<end>':\n",
    "#            return result, attention_plot\n",
    "#\n",
    "#        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "#\n",
    "#    attention_plot = attention_plot[:len(result), :]\n",
    "#    return result, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#while True:\n",
    "#    rid = np.random.randint(0, len(img_name_val))\n",
    "#    image = img_name_val[rid]\n",
    "#    real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\n",
    "#    \n",
    "#    result, attention_plot = evaluate(image)\n",
    "#    prediction_caption = ' '.join(result)\n",
    "#\n",
    "#    real_keywords = {word for word in real_caption.split() if len(word) > 2 and word not in ['<start>', '<end>']}\n",
    "#    predicted_keywords = {word for word in prediction_caption.split() if len(word) > 2 and word not in ['<start>', '<end>']}\n",
    "#    common_keywords = real_keywords.intersection(predicted_keywords)\n",
    "#\n",
    "#    if(len(common_keywords) >= 5):\n",
    "#        print('Image Path:', image)\n",
    "#        print('Real Caption:', real_caption)\n",
    "#        print('Prediction Caption:', prediction_caption)\n",
    "#        print('Common Keywords:', common_keywords)\n",
    "#        plot_attention(image, result, attention_plot)\n",
    "#        break\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataScience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
