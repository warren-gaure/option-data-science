{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73aa0b71",
   "metadata": {},
   "source": [
    "# PIPELINE FINALE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afb0eb9",
   "metadata": {},
   "source": [
    "## Initialisation de la pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7380e84",
   "metadata": {},
   "source": [
    "### 0.1 - Import des librairies nécéssaires au code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d5e646",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import shutil\n",
    "import pickle\n",
    "import collections\n",
    "import random\n",
    "import evaluate as evaluate_metric\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97288056",
   "metadata": {},
   "source": [
    "### 0.2 - Définition des constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3181378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------- General Parameters\n",
    "SEED = 42\n",
    "# -------------------------------- Images Parameters\n",
    "IMAGE_H = 128\n",
    "IMAGE_W = 128\n",
    "BATCH_S = 16\n",
    "# -------------------------------- Classes indexes\n",
    "PAINTING_IDX = 0\n",
    "PHOTO_IDX = 1\n",
    "SCHEMA_IDX = 2\n",
    "SKETCH_IDX = 3\n",
    "TEXT_IDX = 4\n",
    "# -------------------------------- Folders\n",
    "DATASET_DIRECTORY = \"dataset_soutenance/\"\n",
    "PHOTOS_DIRECTORY = \"final_pipeline/photos\"\n",
    "DENOISED_PHOTOS_DIRECTORY = \"final_pipeline/denoised_photos\"\n",
    "MODEL_DIRECTORY = \"models\"\n",
    "# -------------------------------- Model names\n",
    "CLASSIFICATION_MODEL_NAME = \"classification_model.keras\"\n",
    "AUTOENCODER_MODEL_NAME = \"autoencoder_model.keras\"\n",
    "CAPTIONNING_ENCODER_MODEL_NAME = \"encoder.weights.h5\"\n",
    "CAPTIONNING_DECODER_MODEL_NAME = \"decoder_lstm.weights.h5\"\n",
    "CAPTIONNING_TOKENIZER_NAME = \"tokenizer.pickle\"\n",
    "# -------------------------------- Model paths\n",
    "CLASSIFICATION_MODEL_PATH = os.path.join(MODEL_DIRECTORY, CLASSIFICATION_MODEL_NAME)\n",
    "AUTOENCODER_MODEL_PATH = os.path.join(MODEL_DIRECTORY, AUTOENCODER_MODEL_NAME)\n",
    "CAPTIONNING_ENCODER_MODEL_PATH = os.path.join(MODEL_DIRECTORY, CAPTIONNING_ENCODER_MODEL_NAME)\n",
    "CAPTIONNING_DECODER_MODEL_PATH = os.path.join(MODEL_DIRECTORY, CAPTIONNING_DECODER_MODEL_NAME)\n",
    "CAPTIONNING_TOKENIZER_PATH = os.path.join(MODEL_DIRECTORY, CAPTIONNING_TOKENIZER_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670ba74c",
   "metadata": {},
   "source": [
    "### 0.3 - Définition des classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee33864",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.keras.utils.register_keras_serializable('CNN_Encoder')\n",
    "class CNN_Encoder(tf.keras.Model):\n",
    "    # Comme les images sont déjà prétraités par InceptionV3 est représenté sous forme compacte\n",
    "    # L'encodeur CNN ne fera que transmettre ces caractéristiques à une couche dense\n",
    "    def __init__(self, embedding_dim, **kwargs):\n",
    "        super(CNN_Encoder, self).__init__(**kwargs)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        # forme après fc == (batch_size, 64, embedding_dim)\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "    def call(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x\n",
    "    def get_config(self):\n",
    "        config = super(CNN_Encoder, self).get_config()\n",
    "        config.update({\n",
    "            'embedding_dim': self.embedding_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf87de22",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.keras.utils.register_keras_serializable('BahdanauAttention')\n",
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        # features(CNN_encoder output) forme == (batch_size, 64, embedding_dim)\n",
    "\n",
    "        # forme de la couche cachée == (batch_size, hidden_size)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "        attention_hidden_layer = tf.nn.tanh(\n",
    "                self.W1(features) + self.W2(hidden_with_time_axis)\n",
    "        )\n",
    "\n",
    "        # Cela vous donne un score non normalisé pour chaque caractéristique de l'image.\n",
    "        score = self.V(attention_hidden_layer)\n",
    "\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        \n",
    "        return context_vector, attention_weights\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(BahdanauAttention, self).get_config()\n",
    "        config.update({\n",
    "            units: self.units,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a40c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.keras.utils.register_keras_serializable('RNN_Decoder')\n",
    "class RNN_Decoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size, use_lstm=False, **kwargs):\n",
    "        super(RNN_Decoder, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.use_lstm = use_lstm\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        if not use_lstm:\n",
    "            self.layer = tf.keras.layers.GRU(\n",
    "                self.units,\n",
    "                return_sequences=True,\n",
    "                return_state=True,\n",
    "                activation='tanh',\n",
    "                recurrent_activation='sigmoid',\n",
    "                use_bias=True,\n",
    "                kernel_initializer='glorot_uniform',\n",
    "                recurrent_initializer='orthogonal',\n",
    "                bias_initializer='zeros',\n",
    "                unroll=True,\n",
    "            )\n",
    "        else:\n",
    "            self.layer = tf.keras.layers.LSTM(\n",
    "                self.units,\n",
    "                return_sequences=True,\n",
    "                return_state=True,\n",
    "                activation='tanh',\n",
    "                recurrent_activation='sigmoid',\n",
    "                use_bias=True,\n",
    "                kernel_initializer='glorot_uniform',\n",
    "                recurrent_initializer='orthogonal',\n",
    "                bias_initializer='zeros',\n",
    "                unroll=True,\n",
    "            )\n",
    "\n",
    "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "        self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "    def call(self, x, features, hidden):\n",
    "        context_vector, attention_weights = self.attention(features, hidden)\n",
    "        x = self.embedding(x)\n",
    "        context_vector = tf.expand_dims(context_vector, 1)\n",
    "        \n",
    "        x = tf.concat([context_vector, x], axis=-1)\n",
    "\n",
    "        if not self.use_lstm:\n",
    "            x = self.fc1(x)\n",
    "            output, state = self.layer(x)\n",
    "            y = tf.reshape(output, (-1, output.shape[2]))\n",
    "        else:\n",
    "            output, state, _ = self.layer(x)\n",
    "            y = self.fc1(output)\n",
    "            y = tf.reshape(y, (-1, y.shape[2]))\n",
    "\n",
    "        y = self.fc2(y)\n",
    "\n",
    "        return y, state, attention_weights\n",
    "\n",
    "    def reset_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units))\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(RNN_Decoder, self).get_config()\n",
    "        config.update({\n",
    "            'embedding_dim': self.embedding_dim,\n",
    "            'units': self.units,\n",
    "            'vocab_size': self.vocab_size,\n",
    "            'use_lstm': self.use_lstm\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be810f5",
   "metadata": {},
   "source": [
    "### 0.4 - Définition des fonctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f38c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dd2746",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Définition de la fonction load_image\n",
    "def load_image(image_path):\n",
    "    \"\"\"\n",
    "    La fonction load_image a pour entrée le chemin d'une image et pour sortie un couple\n",
    "    contenant l'image traitée ainsi que son chemin d'accès.\n",
    "    La fonction load_image effectue les traitement suivant:\n",
    "        1. Chargement du fichier correspondant au chemin d'accès image_path\n",
    "        2. Décodage de l'image en RGB.\n",
    "        3. Redimensionnement de l'image en taille (299, 299).\n",
    "        4. Normalisation des pîxels de l'image entre -1 et 1\n",
    "    \"\"\"\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (299, 299))\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    return img, image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f3d6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_image(filename):\n",
    "    try:\n",
    "        with Image.open(filename) as img:\n",
    "            img.verify()\n",
    "        return True\n",
    "    except (UnidentifiedImageError, OSError):\n",
    "        return False\n",
    "def move_non_images(directory):\n",
    "    dump_directory = \"dump\"\n",
    "    os.makedirs(dump_directory, exist_ok = True)\n",
    "    \n",
    "    for folder, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(folder, file)\n",
    "            if not is_image(file_path):\n",
    "                print(f\"Déplacement de {file_path} dans le dossier dump/\")\n",
    "                dest_path = os.path.join(dump_directory, file)\n",
    "                try:\n",
    "                    shutil.move(file_path, dest_path)\n",
    "                except:\n",
    "                    print(\"Erreur lors du déplacement\")\n",
    "def is_valid_image(path):\n",
    "    try:\n",
    "        img_raw = tf.io.read_file(path)\n",
    "        _ = tf.image.decode_image(img_raw, channels=3)\n",
    "        return (path, True)\n",
    "    except Exception:\n",
    "        return (path, False)\n",
    "def clean_corrupted_images(directory, extensions=(\"jpg\", \"jpeg\", \"png\"), max_workers=8):\n",
    "    image_paths = []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(extensions):\n",
    "                image_paths.append(os.path.join(root, file))\n",
    "\n",
    "    print(f\"Scan de {len(image_paths)} images dans {directory}\")\n",
    "\n",
    "    corrupted_count = 0\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [executor.submit(is_valid_image, path) for path in image_paths]\n",
    "        for future in as_completed(futures):\n",
    "            path, is_valid = future.result()\n",
    "            if not is_valid:\n",
    "                try:\n",
    "                    os.remove(path)\n",
    "                    corrupted_count += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Erreur de suppression {path} : {e}\")\n",
    "\n",
    "    print(f\"Vérification terminée : {corrupted_count} image(s) corrompue(s) supprimée(s).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bd0b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculMetrics(y_true, y_pred):\n",
    "    # Convertissez y_true, y_pred et all_images en tableaux NumPy pour une manipulation plus facile\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    # si on ne réimporte pas on as une erreur quand on relance la cellule\n",
    "    from tensorflow.keras.metrics import Precision, Recall, Accuracy\n",
    "\n",
    "    Precision = Precision()\n",
    "    Recall = Recall()\n",
    "    Precision.update_state(y_true, y_pred)\n",
    "    Recall.update_state(y_true, y_pred)\n",
    "    Accuracy = Accuracy()\n",
    "    Accuracy.update_state(y_true, y_pred)\n",
    "    F1Score = 2 * ((Precision.result() * Recall.result()) / (Precision.result() + Recall.result()))\n",
    "    return {\"Precision\": Precision.result(), \"Recall\": Recall.result(), \"Accuracy\": Accuracy.result(), \"F1Score\": F1Score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73fc37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_matrix(y_pred, y_true, class_names):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    display = ConfusionMatrixDisplay(cm, display_labels = class_names)\n",
    "    display.plot(cmap = plt.cm.Blues)\n",
    "    plt.title(\"Matrice de confusion\")\n",
    "    plt.xticks(rotation = 45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af16da9f",
   "metadata": {},
   "source": [
    "## Partie 1 : Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eff0705",
   "metadata": {},
   "source": [
    "### 1.0 - Mise au propre des folders & tri des fichiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77059ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# move_non_images(DATASET_DIRECTORY)\n",
    "\n",
    "# clean_corrupted_images(DATASET_DIRECTORY)\n",
    "\n",
    "# Remove Photos from previous iteration\n",
    "if not os.path.exists('final_pipeline'):\n",
    "    os.mkdir('final_pipeline')\n",
    "for dir in [\n",
    "    PHOTOS_DIRECTORY, \n",
    "    DENOISED_PHOTOS_DIRECTORY,\n",
    "    ]:  \n",
    "    # Remove the directory if it exists\n",
    "    if os.path.exists(dir):\n",
    "        shutil.rmtree(dir) \n",
    "    # Create the directory\n",
    "    if not os.path.exists(dir):\n",
    "        os.mkdir(dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8a99df",
   "metadata": {},
   "source": [
    "### 1.1 - Import des données source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eeea67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = keras.utils.image_dataset_from_directory(\n",
    "    DATASET_DIRECTORY,\n",
    "    batch_size = BATCH_S,\n",
    "    image_size = (IMAGE_H, IMAGE_W),\n",
    "    label_mode = None,\n",
    "    seed = 42,\n",
    "    validation_split = None,\n",
    "    subset = None,\n",
    "    shuffle = None,\n",
    ")\n",
    "filepaths = dataset.file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c90fed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "labels = [os.path.split(filepath)[-1] for filepath in filepaths]\n",
    "labels = [label.split(\".\")[0].split(\" \")[0].split(\"-\")[0] for label in labels]\n",
    "truelabels = []\n",
    "for label in labels:\n",
    "    label = label.lower()\n",
    "    if label == \"photo\":\n",
    "        truelabels.append(PHOTO_IDX)\n",
    "    elif label == \"paint\":\n",
    "        truelabels.append(PAINTING_IDX)\n",
    "    elif label == \"schematics\":\n",
    "        truelabels.append(SCHEMA_IDX)\n",
    "    elif label == \"text\":\n",
    "        truelabels.append(TEXT_IDX)\n",
    "    elif label == \"wp\" or label == \"zmp\":\n",
    "        truelabels.append(SKETCH_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8b6d01",
   "metadata": {},
   "source": [
    "### 1.2 - Classification des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6896ae9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model = load_model(CLASSIFICATION_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec360e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = classification_model.predict(dataset, verbose = 1)\n",
    "y_pred = []\n",
    "y_pred.extend(predicts.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c2180b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_matrix(y_pred, truelabels, ['Painting', 'Photo', 'Schematics', 'Sketch', 'Text'])\n",
    "metrics = calculMetrics(truelabels, y_pred)\n",
    "for key, value in metrics.items():\n",
    "    print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f664afc6",
   "metadata": {},
   "source": [
    "### 1.3 - Copie des photos dans un répertoire spécifique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d1fc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_preds = list(zip(filepaths, y_pred))\n",
    "photos_preds = list(filter(lambda x: x[1] == PHOTO_IDX,images_preds))\n",
    "\n",
    "for filepath, prediction in images_preds:\n",
    "    if prediction == PHOTO_IDX:\n",
    "        filename = os.path.basename(filepath)\n",
    "        # print(filename)\n",
    "        dest_path = os.path.join(PHOTOS_DIRECTORY, filename)\n",
    "        shutil.copy(filepath, dest_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616ef78b",
   "metadata": {},
   "source": [
    "## Partie 2 : Dénoising des images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc0c1e7",
   "metadata": {},
   "source": [
    "### 2.0 - Import des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f256497b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = keras.utils.image_dataset_from_directory(\n",
    "    directory = PHOTOS_DIRECTORY,\n",
    "    batch_size = BATCH_S,\n",
    "    image_size = (IMAGE_H, IMAGE_W),\n",
    "    label_mode = None,\n",
    "    seed = 42,\n",
    "    validation_split = None,\n",
    "    subset = None,\n",
    "    shuffle = None,\n",
    ")\n",
    "filepaths = dataset.file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755399d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda x: tf.cast(x, tf.float32) / 255.0)\n",
    "X = []\n",
    "for batch in dataset:\n",
    "    X.append(batch.numpy())\n",
    "dataset = np.concatenate(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54991a94",
   "metadata": {},
   "source": [
    "### 2.1 - Denoising des images sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce74e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model from file\n",
    "autoencoder_model = load_model(AUTOENCODER_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4aa9589",
   "metadata": {},
   "outputs": [],
   "source": [
    "denoised_images = autoencoder_model.predict(dataset, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fe63b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(denoised_images[1])\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2cb1c8",
   "metadata": {},
   "source": [
    "### 2.2 - Sauvegarde des images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c30cbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save images from denoised_images\n",
    "for i, image in enumerate(denoised_images):\n",
    "    # Convert the image to uint8 format\n",
    "    image = (image * 255).astype(np.uint8)\n",
    "    # Create a PIL Image from the numpy array\n",
    "    pil_image = Image.fromarray(image)\n",
    "    # Save the image\n",
    "    filename = os.path.basename(filepaths[i])\n",
    "    dest_path = os.path.join(DENOISED_PHOTOS_DIRECTORY, filename)\n",
    "    pil_image.save(dest_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54341d3e",
   "metadata": {},
   "source": [
    "## Partie 3 : Captioning des images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ad686a",
   "metadata": {},
   "source": [
    "### 3.0 - Import des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f0944e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = keras.utils.image_dataset_from_directory(\n",
    "    directory = DENOISED_PHOTOS_DIRECTORY,\n",
    "    batch_size = BATCH_S,\n",
    "    image_size = (IMAGE_H, IMAGE_W),\n",
    "    label_mode = None,\n",
    "    seed = 42,\n",
    "    validation_split = None,\n",
    "    subset = None,\n",
    "    shuffle = True,\n",
    ")\n",
    "filepaths = dataset.file_paths\n",
    "\n",
    "n_samples = 200 if len(filepaths) > 200 else len(filepaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ce973e",
   "metadata": {},
   "source": [
    "### 3.1 - Prétraitement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c5429c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Telechargement du modèle InceptionV3 pré-entrainé avec la cassification sur ImageNet\n",
    "image_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n",
    "# Creation d'une variable qui sera l'entrée du nouveau modèle de pre-traitement d'images\n",
    "new_input = image_model.input\n",
    "# récupérer la dernière couche caché qui contient l'image en representation compacte\n",
    "hidden_layer = image_model.layers[-1].output\n",
    "\n",
    "# Modèle qui calcule une representation dense des images avec InceptionV3\n",
    "image_features_extract_model = tf.keras.Model(inputs = new_input, outputs = hidden_layer)\n",
    "\n",
    "\n",
    "# Pré-traitement des images\n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(filepaths[:n_samples])\n",
    "# Division du données en batchs après application du pré-traitement fait par load_image\n",
    "image_dataset = image_dataset.map(\n",
    "  load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(16)\n",
    "\n",
    "# Parcourir le dataset batch par batch pour effectuez le pré-traitement d'InceptionV3\n",
    "for img, path in tqdm(image_dataset):\n",
    "    # Pré-traitement du batch (de taille (16,8,8,2048)) courant par InceptionV3 \n",
    "    batch_features = image_features_extract_model(img)\n",
    "    # Resize du batch de taille (16,8,8,2048) en taille (16,64,2048)\n",
    "    batch_features = tf.reshape(batch_features,\n",
    "                              (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "    # Parcourir le batch courant et stocker le chemin ainsi que le batch avec np.save()\n",
    "    for bf, p in zip(batch_features, path):\n",
    "        path_of_feature = p.numpy().decode(\"utf-8\")\n",
    "        # (chemin de l'image associe a sa nouvelle representation , representation de l'image)\n",
    "        np.save(path_of_feature, bf.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70eb734a",
   "metadata": {},
   "source": [
    "### 3.2 - Import des classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f2226b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "max_length = 15\n",
    "features_shape = 2048\n",
    "attention_features_shape = 64\n",
    "\n",
    "with open(CAPTIONNING_TOKENIZER_PATH, 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "vocab_size = 5000 + 1\n",
    "\n",
    "\n",
    "encoder = CNN_Encoder(embedding_dim)\n",
    "decoderLSTM = RNN_Decoder(embedding_dim, units, vocab_size, use_lstm=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ec00aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the models by calling them with dummy data\n",
    "dummy_image_features = tf.random.normal([1, attention_features_shape, features_shape])\n",
    "dummy_caption_input = tf.random.uniform([1, 1], minval=0, maxval=vocab_size, dtype=tf.int32)\n",
    "dummy_hidden_state = tf.zeros((1, units))\n",
    "\n",
    "# Build the encoder by calling it with dummy data\n",
    "_ = encoder(dummy_image_features)\n",
    "\n",
    "# Build the decoder by calling it with dummy data\n",
    "dummy_encoder_output = encoder(dummy_image_features)\n",
    "_ = decoderLSTM(dummy_caption_input, dummy_encoder_output, dummy_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dfacb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.load_weights(CAPTIONNING_ENCODER_MODEL_PATH)\n",
    "decoderLSTM.load_weights(CAPTIONNING_DECODER_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bf214b",
   "metadata": {},
   "source": [
    "### 3.3 Génération des captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6860b736",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(image, decoder):\n",
    "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
    "\n",
    "    hidden = decoder.reset_state(batch_size=1)\n",
    "\n",
    "    temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
    "    img_tensor_val = image_features_extract_model(temp_input)\n",
    "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "\n",
    "    features = encoder(img_tensor_val)\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
    "    result = []\n",
    "\n",
    "    for i in range(max_length):\n",
    "        try:\n",
    "            predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
    "\n",
    "            attention_plot[i] = tf.reshape(attention_weights, (-1,)).numpy()\n",
    "\n",
    "            predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
    "            \n",
    "            result.append(tokenizer.index_word[predicted_id])\n",
    "        \n",
    "            if tokenizer.index_word[predicted_id] == '<end>':\n",
    "                break\n",
    "            \n",
    "            dec_input = tf.expand_dims([predicted_id], 0)\n",
    "        except KeyError:\n",
    "            print(\"KeyError: predicted_id not in tokenizer\")\n",
    "\n",
    "    attention_plot = attention_plot[:len(result), :]\n",
    "    return result, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6977fa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour valider une légende\n",
    "def is_valid_caption(caption):\n",
    "    words = caption.split()\n",
    "\n",
    "    # Critère 1 : au moins 5 mots\n",
    "    if len(words) < 5:\n",
    "        return False\n",
    "\n",
    "    # Critère 2 : au moins 3 mots de au moins 5 lettres\n",
    "    long_words = [w for w in words if len(w) >= 5]\n",
    "    if len(long_words) < 3:\n",
    "        return False\n",
    "\n",
    "    # Critère 3 : pas plus de 2 occurrences du même mot\n",
    "    lower_words = [w.lower() for w in words]\n",
    "    word_counts = collections.Counter(lower_words)\n",
    "    if any(count > 2 for count in word_counts.values()):\n",
    "        return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7664f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_captions = {}\n",
    "with open(DATASET_DIRECTORY + \"captioning.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        image_path, caption = line.strip().split(\":\")\n",
    "        image_path = image_path.strip()\n",
    "        caption = caption.strip()\n",
    "        real_captions[image_path] = caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33b08c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = random.choice(filepaths[:n_samples])\n",
    "image_name = os.path.split(image)[-1]\n",
    "image_name = image_name.replace(\"  \", \" \")\n",
    "\n",
    "clean_img = image.replace(\"denoised_photos\", \"photos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521f4441",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choisir une image aléatoire\n",
    "valid_captions_LSTM = []\n",
    "\n",
    "# Boucle jusqu'à obtenir 5 captions valides par modèle\n",
    "while len(valid_captions_LSTM) < 5 :\n",
    "    result_LSTM, attention_plot_LSTM = evaluate(image, decoderLSTM)\n",
    "\n",
    "    predicted_caption_LSTM = ' '.join([word for word in result_LSTM if word not in ['<start>', '<end>']])\n",
    "\n",
    "    if len(valid_captions_LSTM) < 5 and is_valid_caption(predicted_caption_LSTM):\n",
    "        valid_captions_LSTM.append((predicted_caption_LSTM, attention_plot_LSTM))\n",
    "\n",
    "# Affichage final\n",
    "print('Image Path:', image)\n",
    "if image_name in real_captions.keys() : print('Real Caption:', real_captions[image_name])\n",
    "\n",
    "# Affichage LSTM\n",
    "for i, (caption, attn) in enumerate(valid_captions_LSTM, 1):\n",
    "    print(f'LSTM Prediction {i}:', caption)\n",
    "    #plot_attention(image, caption.split(), attn)\n",
    "\n",
    "# Afficher l'image originale\n",
    "img = Image.open(image)\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Image utilisée\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1118447",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choisir une image aléatoire\n",
    "valid_captions_LSTM = []\n",
    "\n",
    "# Boucle jusqu'à obtenir 5 captions valides par modèle\n",
    "while len(valid_captions_LSTM) < 5 :\n",
    "    result_LSTM, attention_plot_LSTM = evaluate(clean_img, decoderLSTM)\n",
    "\n",
    "    predicted_caption_LSTM = ' '.join([word for word in result_LSTM if word not in ['<start>', '<end>']])\n",
    "\n",
    "    if len(valid_captions_LSTM) < 5 and is_valid_caption(predicted_caption_LSTM):\n",
    "        valid_captions_LSTM.append((predicted_caption_LSTM, attention_plot_LSTM))\n",
    "\n",
    "# Affichage final\n",
    "print('Image Path:', clean_img)\n",
    "if image_name in real_captions.keys() : print('Real Caption:', real_captions[image_name])\n",
    "\n",
    "# Affichage LSTM\n",
    "for i, (caption, attn) in enumerate(valid_captions_LSTM, 1):\n",
    "    print(f'LSTM Prediction {i}:', caption)\n",
    "\n",
    "# Afficher l'image originale\n",
    "img = Image.open(clean_img)\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Image utilisée\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f627de",
   "metadata": {},
   "source": [
    "### 3.4 - Affichage des metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3f77f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "bleus = []\n",
    "rouges = []\n",
    "meteors = []\n",
    "CIDErs= []\n",
    "real_caption = real_captions[image_name]\n",
    "\n",
    "#bleu = evaluate_metric.load(\"bleu\", verbose=False)\n",
    "meteor = evaluate_metric.load(\"meteor\", verbose=False)\n",
    "rouge = evaluate_metric.load(\"rouge\", verbose=False)\n",
    "#cider_scorer = Cider()\n",
    "for i, (caption, attn) in enumerate(valid_captions_LSTM, 1):\n",
    "    predictions = [caption]\n",
    "    references = [[real_caption]]\n",
    "\n",
    "    # BLEU\n",
    "    smooth = SmoothingFunction().method4\n",
    "    score = sentence_bleu(caption.split(), real_caption.split(), smoothing_function=smooth)\n",
    "    bleus.append(score)\n",
    "\n",
    "    # METEOR\n",
    "    meteors.append(meteor.compute(predictions=predictions, references=references)[\"meteor\"])\n",
    "\n",
    "    # ROUGE\n",
    "    rouges.append(rouge.compute(predictions=predictions, references=references)[\"rougeLsum\"])\n",
    "\n",
    "    gts = {\n",
    "        \"img1\": [{\"caption\": real_caption}],\n",
    "    }\n",
    "    res = {\n",
    "        \"img1\": [{\"caption\": caption[0]}],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289f801f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [\"caption \" + str(i) for i in range(len(bleus))]\n",
    "metrics = [bleus, rouges, meteors] #, CIDErs]\n",
    "labels = ['BLEU', 'ROUGE', 'METEOR'] #, 'CIDEr']\n",
    "colors = ['blue', 'red', 'green'] #, 'orange']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bottom = np.zeros(len(x))  # Hauteur de départ pour chaque segment\n",
    "\n",
    "for i, (metric, name, color) in enumerate(zip(metrics, labels, colors)):\n",
    "    bars = plt.bar(x, metric, bottom=bottom, label=name, color=color)\n",
    "\n",
    "    # Annoter chaque segment\n",
    "    for xi, yi, base in zip(x, metric, bottom):\n",
    "        if yi > 0:\n",
    "            plt.text(xi, base + yi / 2, f'{yi:.2f}', ha='center', va='center', fontsize=9, color='white')\n",
    "\n",
    "    # Mise à jour de la hauteur de base\n",
    "    bottom += metric\n",
    "\n",
    "plt.ylabel('Score')\n",
    "plt.title('Métrics empilés par caption')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
